{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_x9N13AKyiu",
    "outputId": "2ce5ed28-84f7-456a-da1e-30b462a6edcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    " # Need to install from github, for staying up-to-date with the latest developments.\n",
    "  # reason: if a bug has been fixed since the last official release but a new release hasn’t been rolled out yet\n",
    "%pip -q install git+https://github.com/huggingface/transformers\n",
    "# %pip -q install transformers\n",
    "%pip -q install -U datasets\n",
    "%pip -q install -U loralib\n",
    "%pip -q install -U sentencepiece\n",
    "%pip -q install -U bitsandbytes\n",
    "%pip -q install -U accelerate\n",
    "%pip -q install -U einops\n",
    "%pip -q install -U langchain\n",
    "%pip -q install -U huggingface_hub\n",
    "%pip -q install -U chromadb\n",
    "%pip -q install -U PyPDF2\n",
    "%pip -q install -U pypdf\n",
    "%pip -q install -U sentence-transformers\n",
    "%pip -q install -U FlagEmbedding\n",
    "%pip -q install -U InstructorEmbedding\n",
    "\n",
    "# %pip -q install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZEstQqUUp7C"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYev9nnWDRZy"
   },
   "source": [
    "###Download the PDFs as external resourses\n",
    "\n",
    "This part show that we can load a link and extract many pdf files, even the ones with misinformation included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H63WS5G-DRCo",
    "outputId": "f1910162-812c-4f69-bb87-9b93f7ab3e1e"
   },
   "outputs": [],
   "source": [
    "!wget -O PDFPapers.zip your_path_to_the_zip_file\n",
    "!unzip -q PDFPapers.zip -d papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "23bf45437ce6409184b1c0e466a7101e",
      "9f1a6f50d39d4df2ad173aa8d9b41e41",
      "1887216f85004954b9634c9124b1bac9",
      "efa4e99c9283460288842dca78ad4d01",
      "1261f37570bf4257ad55024081999409",
      "ff32fb6fb1b4459caaf4a5c48b8014af",
      "1293beb5c95e4a1c9633630f5bb9bea1",
      "fb84cfd19fb64242a78bb732f63316d1",
      "df5120c70209444cb482bb0ef5c37318",
      "d696d410a7df45b1b242c8714dccb940",
      "b88ffe2f67584f05bcefa2c04084b274",
      "55a39190b648444d99bc71fb0d51cbc0",
      "3bf27823ccdb4a938bbfb7c2c1dc618a",
      "e08f4ad0e0ef4ab3976d987aa385d61c",
      "3f9a074ac29f4ba5842218710d4103ec",
      "7c3e27a4c8934a0690bb64d69db07782",
      "c268690ebd1743c1842173ba0e8de734",
      "7a89a08a0c7447f8b1210dcc6bf93b6a",
      "0b5f660ff9274e449ce9fb87249aaa29",
      "0366b3c26cdb4b7d8fab9f3c1de29562",
      "5fbbb906848b4c4e84995f5131bb09da",
      "1f281e3364fd4573b249b9df410c151f",
      "521bc82b39b34f4a8bea33cc0d27a8af",
      "edd49ea938504249bc7548060f782edf",
      "f568b16fcaaf482b934cd0d13917d1c1",
      "bcbd136c794a423798e15d3c42bcb57a",
      "52698b73fe2c4c99bdb1bf05d6568766",
      "d9aa6637755240d2aa28736296f0acf2",
      "ccdbca3b36854de4aabea2706d26ae7f",
      "b76acac829c541ad866ea86517c6eebb",
      "9482d7c593474010b0a0ac44a3a81794",
      "aaa73632195c44cd9273a81b196bbb31"
     ]
    },
    "id": "hebEJuaGT_Fb",
    "outputId": "9d6ab9d1-9462-478d-b5c5-2dea212df2a5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bf45437ce6409184b1c0e466a7101e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpC_r9cT6E74"
   },
   "source": [
    "### Frameworks/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRswMcgcT_JP"
   },
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "# import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "# LangChain\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# embeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# 'mixedbread-ai/mxbai-embed-large-v1' embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# to create chain\n",
    "from langchain.schema import prompt\n",
    "\n",
    "# formatting the response\n",
    "# import json\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O74ftbYf6MKt"
   },
   "source": [
    "### Loading Model, and its Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612,
     "referenced_widgets": [
      "fec3bd85c52f46e3b8d282f48a0ff4af",
      "2c902a30db4a4f66b98d6eb66e6327f1",
      "740bfdf0f72a45d995059196e9622ed3",
      "ced3877cabf44ab5a212c9970b569a77",
      "0e7246f9a63e43f6b7890afd2764f384",
      "dd8a972f36474e559a2d4fbb48c5ba30",
      "01dd1005d2354f1ebd4c61003aee2d32",
      "dbd407a5d75145a2a3cbbfabc9f96c71",
      "d33b2334497e48cdb5f02a8283a199f2",
      "46716d7d7eb74d0ca5e857c425239436",
      "e97c7b2fdeb24d319523fc5d9314293d",
      "71b451fca481469ea6794666f857c99b",
      "7b0bbb0247734ca6881405c80a235215",
      "f2221eb86b7e483fa92b822e993fd85a",
      "ddce4c5d6ed44cda9db14a5181077474",
      "6e83e74b9fcc49c088c4f95c61cba3d0",
      "80fde45cbe9e4284bd7c6be0e0f5610c",
      "573236e4640f4304b1727190f2777732",
      "43fce2542dd8448cb2c48e99f0a80d6c",
      "1b30775c9c204ca082fdd88740e648ef",
      "e4420be8796a443d9eb00f2a10efe8dd",
      "574f4c8c94024bc5ba1946a4205b38ce",
      "1c60b47bff9d44eeb67d8e0b5c2f7d15",
      "037a554ccbbc41ea9e970157bc7eb0d7",
      "2917d9c755be4397b03ad2f78f1abf79",
      "f1bead4e812643db9e3c0af8e876f386",
      "3c5cb0bff86d474bab14d1e8452c03fe",
      "002041900b604adab98d696c7198ed0e",
      "64000322383c4f57acff83622c7b358d",
      "f72a4f970502478c83fa1d0174eb263c",
      "60d00a2405cf4350ac25aa8907d4a0c4",
      "e101d1fcfadc4e17a344d715922154e3",
      "15513d3703a243f2a04f33aa1402d476",
      "9bfd503d74ae4065b762864bae727130",
      "c7cfe4b7ff16427d89dec514c0aeb92d",
      "e45070a7b6b647238075e8be25d35641",
      "fbead7b64c7c40c190ff6cc5f0131943",
      "1a1495f8a19d4c81b737dd5c2e47b857",
      "9654821dc6374bcf83ecf9f116227709",
      "f706032fe5cd4b57b6fe0895d96b2237",
      "7b8e53eac3594dd98d36f36cc9f052d2",
      "e3c87d1565fa41e4ab3efaeb4b58b58c",
      "6d6e605d7b66433eb523cb67efb93c60",
      "14af104bf8a8453d8108ac7721c83315",
      "6ca97e66d96e4d0ea5a97d36746ec95e",
      "b296da6b5deb4eb0921abf2d24fa37c4",
      "222f8492a9cd42a1bebd500c51fa0bd5",
      "cf174d180b554d1aa4dec34999bb1c88",
      "d6d709829cd74ad3a9c8c6829a81f9b2",
      "86752b76123b40a7ac9850c47245d111",
      "e5c39b3b24b04a7c967aed2338656619",
      "a8cea687662b427ba5fd5d998d8cb5b3",
      "bdccd1cdcf354119a16cb430cea5643a",
      "34797c85fea64a51ab8fb3bb045688c8",
      "2aa5bee586eb45879006f2420d2a3628",
      "ea0eedfed23a4ba5876111d7068da537",
      "28751b14571e4946abe862996342f7b0",
      "84a66d28b4fa4d41a14efb9ad7864310",
      "f02af320eccf4b8bb5a8a434afdceb4e",
      "b641ab0c056b4104a40e671a0474f293",
      "964d30c202fb492c9fe60e3dd1af844e",
      "a688691934e9497ca378c472a8460d20",
      "c64786bdb6cf4254a75fbb20fb2765a2",
      "bd80a7b9d5fe432d9809c31fe7d16259",
      "e1c68a8c5e0942e2a44a198f8044d76e",
      "87c75806934643a7a46518d968e83320",
      "beafbbad2e1943708c82c7fec4f048ee",
      "3bbacd04642045129f5a5fcdc30630a6",
      "d9ff912bd9bb463f878af20f9240041a",
      "f98abe30a22f49f5ace37e5d20ccee37",
      "3400ff0f60634591a5867a1b762d78d2",
      "1e886f29300f441d99273ce878d3289a",
      "2c9237761bf84fee8650be3da0c08494",
      "936a018b20694922997c6bcfa37c2054",
      "cd1f6d87f0234baeb5e6d3143dc196aa",
      "05cd8e54f2e94faebf4c2e227fae1e5f",
      "a747a39bb04546248526684c105179ba",
      "b162f44d2069466ea3e9103093364bb5",
      "5a98451db7b14bdbb54f9eeab5f8da19",
      "e3dec87ca68845d1a41c06764cdc7d69",
      "196a3a27115647cf8f1734d17116b756",
      "19c35688e3194214926658520a45f169",
      "3ade9c7856d744c3af4ed7b335b7189e",
      "52a444ea63e0441a8ea79c936d52eb0a",
      "6186be01eb8f47df9762083f6ff80749",
      "9df87b2000284e01b4db02d1666fb76f",
      "8df34d2eea884ca18bab1c40ae703caa",
      "620d5322a17e403eb005a1782886b60f",
      "c46c5f1c852249ef830544dc6477624d",
      "be98e7bc5873401e9b2d105e12d166f5",
      "0ade2d279bcf41839323bb1273f29c4c",
      "8909de6fdf1543fd8c6d01f11fe375a7",
      "589fa8ee44a6409a838477e50e55e57d",
      "1fc9dec7f7494d1eabd03c016e09da60",
      "826e0e1ae457443899981833aa7ec5d0",
      "426440bd10d0467cb8eea069b1b8d905",
      "927a9a4061bf4b6fa5f0d252bd131c14",
      "778db3a016c949008d04f5c4f2a2aff9",
      "59fd6bdb84b3422085a4e701f2f930a0",
      "9879926d07ee4fdc9d7b9a9c17165066",
      "5260c96059654a80abaa37d36707b7ab",
      "e31dca8e9071462fbcce4065049a1fc4",
      "ae4e065cbcd84982a2c3adac7f482663",
      "27c4963d070b4c38a5875e71406fd96e",
      "9b0ae08a278642239b84ee6ae17229a3",
      "f06b79cadbe148f0a6d9326dc0c124ca",
      "be25b63783824d9cbb3b100c446476c0",
      "046654fefe8344f9a2b3efd928c16af9",
      "a24de36882e54e1b86aee7adcd27ec1e",
      "2adf960b5bc04fa68fb0986d8335363f",
      "2c6033d8cbf5411ca3bfa297c5f44ef1",
      "d5b68083538f43418f2cd1c03224d086",
      "6cdc3b2fc088489487261958c002b01f",
      "f543043a87fc4c64a8c882d18e0eb2dc",
      "20b8b2cb8804407cbce682dd97cce234",
      "33524de32bda4d65b24cf8b26430f1cf",
      "5e6486e79e7d4a2c8a42eb6c0786671f",
      "ba491ed1463c4b398cca28c11a140c4f",
      "5673b053c6e940c78e849fc9106b8edd",
      "0498c67074714e4cbe3f0e1b07295907",
      "2463bd8fae3145c18ea6b31689881628",
      "bbbe9b65796d40e19a2a38f90661ea9d",
      "51e3d40641af49dd9ce01f5c588b930a",
      "7d00a752e6aa4cbb963ffd840af71866",
      "873b1c759898413fb0f13db922aa2635",
      "61e4132b12f2493489df1e2535ddd445",
      "a84fd92fcbe645b789c251664db8f0c5",
      "d15dd441080f4b06a8246508ab0a4fa4",
      "06f45986391047708d004c3d6056aa32",
      "ddb3a548269a48d3b4b467e780c197fb",
      "e04fc4738e384cb793bd96eaa813ad03",
      "d38ed742a3384fd59bcdeabe54a21a4c"
     ]
    },
    "id": "IhXWIiuvT_M_",
    "outputId": "ff745de8-733b-458b-abd9-d9a1c6071e88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:758: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec3bd85c52f46e3b8d282f48a0ff4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b451fca481469ea6794666f857c99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c60b47bff9d44eeb67d8e0b5c2f7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfd503d74ae4065b762864bae727130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca97e66d96e4d0ea5a97d36746ec95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0eedfed23a4ba5876111d7068da537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beafbbad2e1943708c82c7fec4f048ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b162f44d2069466ea3e9103093364bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46c5f1c852249ef830544dc6477624d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9879926d07ee4fdc9d7b9a9c17165066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6033d8cbf5411ca3bfa297c5f44ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbe9b65796d40e19a2a38f90661ea9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer that correspond to the model, used to convert text to a fromat that model can understand(tokenization) and back to the text(detokenization)\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name,\n",
    "                                          use_auth_token = True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
    "                                             device_map = 'auto',\n",
    "                                             torch_dtype = torch.float16,\n",
    "                                            #  use_auth_token = True,\n",
    "                                             load_in_8bit=True # 8bit/4bit\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htViqv4RV_5J"
   },
   "source": [
    "###Building Pipeline\n",
    "\n",
    "“Max Length” controls the overall length of the response.(restricts the total length (input + output))\n",
    "\n",
    "“Max New Tokens” specifically limits the tokens generated beyond the input. It ensures that the output aligns with your desired length while considering the context provided.(specifically limits the tokens generated beyond the input)\n",
    "\n",
    "\n",
    "https://medium.com/@developer.yasir.pk/understanding-the-controllable-parameters-to-run-inference-your-large-language-model-**30643bb46434**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2HNTY9vT_QQ"
   },
   "outputs": [],
   "source": [
    "# To create a text generation pipeline\n",
    "\n",
    "# pipelie(): The pipeline is a high-level utility that simplifies the usage of Transformer models for various tasks, such as text generation\n",
    "# do_sample: Enables sampling, this allows the model to generate text probabilistically rather than deterministically. Sampling can lead to more varied and interesting outputs\n",
    "# top_k: Sample from the top k most likely next tokens at each step, This helps in reducing the randomness of the output, providing a balance between creativity and coherence\n",
    "# eos_token_id: specify the token that indicates the end of a sequence, Allowing the model to determine when to stop generating further tokens\n",
    "\n",
    "# \"text-classification\"\n",
    "pipe = pipeline(\"text-generation\", # specify the task for the pipeline\n",
    "               model = model,\n",
    "               tokenizer = tokenizer,\n",
    "              #  torch_dtype = torch.bfloat16, # data type for PyTorch tensors\n",
    "                max_length=1024,\n",
    "                temperature=0.1,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15,\n",
    "                max_new_tokens=512,\n",
    "              #  device_map = 'auto',\n",
    "              #  do_sample = True,\n",
    "              #  top_k = 30,\n",
    "              #  num_return_sequences = 1, # only one text sequence should be return for each input\n",
    "              #  eos_token_id = tokenizer.eos_token_id\n",
    "               )\n",
    "hf_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V0ozFbAT_Tt"
   },
   "outputs": [],
   "source": [
    "# print(hf_llm(\"Who are you?\"))\n",
    "# pipe(\"Who are you?\")\n",
    "\n",
    "# sequences = pipe(\"Who are you\")\n",
    "# for seq in sequences:\n",
    "#   print(f\"reuslts: {seq['generated_text']}\")\n",
    "\n",
    "# pipe(\"I'm in a good mood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC1tACg3T_XI"
   },
   "outputs": [],
   "source": [
    "# tokenzier.vocab_size\n",
    "# tokenizer.all_special_tokens\n",
    "# tokenizer.all_special_ids\n",
    "# tokenizer(['<unk>'])\n",
    "# tokenizer(['<SYS>\\n'])\n",
    "# tokenizer.decode([1, 14816, 29903, 6778, 13]) # output: '<s>SYS>>\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuLgUewuJjVP"
   },
   "source": [
    "###Setting up Langchain to retrieve PDFs\n",
    "\n",
    "Load and process PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hibbLkK-Jigw",
    "outputId": "3842594c-9257-4078-ca03-ab48952d69b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process a single text files\n",
    "# loader = TextLoader('single_text_file.txt')\n",
    "loader = DirectoryLoader('/content/papers', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2bpEn5yKUlG",
    "outputId": "0bbe1ebc-dc84-409f-def9-22b878e39e3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the text into chunks\n",
    "# chunk_overlap: if we get one idea between two chunks of text,we want it to be overlapped, so we can actually get that in one full chunk by itself.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh-wOyQIcnK4"
   },
   "source": [
    "###Text Embeddings\n",
    "\n",
    "MTEB is a massive benchmark for measuring the performance of text embedding models on diverse embedding tasks\n",
    "\n",
    "https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MllQCHjnfji1"
   },
   "source": [
    "###BAAI/bge-large-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "0b22492ea20e4ff6ac13c70222956830",
      "ba424ec7d1ef45cc900fea9c24a74898",
      "f679180fd4b7473d94aeacab41bbf015",
      "af940397c311455ea2ffa7b343880c1a",
      "d79442f915ea4c9ca7cb22b96bac263d",
      "c99a95faa5664465a83e968952c89604",
      "4676f34c60d0420996c4a3d1ff571e29",
      "2eccbfca06b848f1945a27c1dbe36249",
      "d240bb0ca648469583326142ceca0580",
      "1781208ee1244bfaa2aecb9dadd9f98e",
      "94ec93a7fddb48d68ee438c65ab9357a",
      "00e1754973524518bb379925783225df",
      "280bcb15edaf43b09d8254700b4d0754",
      "7f5bc57569ca43a68b66f54cc4a201b4",
      "6e3dfa3dd4434e0c8d9926c0a06820b8",
      "b3b03b70cb7347a4863da86daf5c69f1",
      "f83ed961262c46189f7b5f3a38ecb12f",
      "aa4d595f29b649699b206528f88fce04",
      "35fda595e1f548b49774690bf3f6b912",
      "9aae3babe542455281f0966564e6da77",
      "3cf6b832476d48f49f7e8eb914a48337",
      "984c3923b8584edaa32844052f72fb12",
      "448e57cc2ba84a77a605397c5fd2b51f",
      "0da1553d8040458ba762fef23a187d7e",
      "92668dd987054aa599d07f40097d4dbe",
      "c1b5cdd231d04962ac6dad0e1d60e29c",
      "dcd7aed591664d1d8873dbff4304621f",
      "ddbe6fe9fda54943b9621d2ebc023efc",
      "a53038acaf564a08881533914b9df9d7",
      "ab22d7af38b146e8be12c67e1472d500",
      "98169847b24b45b3a4af94efcd6f550a",
      "28f930cc65ea4e9cbaeba3d4160b3f67",
      "1100bd1a3d6a470cbb0040cdf871e130",
      "307eb842339c4fdc94de74509906b632",
      "961e0fc3f38444be85000d38e6a6d1fc",
      "7cce2628fa72424782939699cc18edc9",
      "97c0636c927c4186b8679d25b7176bb4",
      "4986a6a89f8a41888c67757b5bc79862",
      "e4640c0400324cce8234371c100e629c",
      "a6481318fb2e48748917ee42d4b977db",
      "a79cd4416aee42fe94ca6e5494013e95",
      "d83c82cdd70940ecbd96f4ca6ff9dd8b",
      "ba6be75453aa4dfa96f9f037b65ea445",
      "d7ac435f3e1747838bd9530d9eed2ded",
      "f8f3c77757cd4d50b3ef650330381748",
      "5d449ee019d747baaabf5326c3a4259c",
      "7c7c7e624f574219b56e6be2f8181a12",
      "2abd5b29aff74bc18180527e8c4a0a93",
      "8a4df7f105ae4b8d92381c308ac0f917",
      "4addf254069742238761baa022adc5cd",
      "275c5a6424b34d21990382674a3631da",
      "0ef16a9668a94e748d2d90eae4d017b0",
      "8500952c31b847039a71bdee2abd9199",
      "24f0aa7cae3740c4b11aede5b79dfce8",
      "8ee5331d9fea4df5a5a4b43f66f3af72",
      "9b7bc871a2524d39a836ab2d4504a65c",
      "bbd42fc7477449fc9fec199fc199c185",
      "81e14c79bbdd40d387684e1633555dea",
      "e53d692f859046cf9d6cb7a2fa709d66",
      "51b7c70dc4b9450487f8d475980b6e42",
      "ba334a5029914c5c8d2ea654774632c6",
      "6bf16cc4619f417e89469699865ea4e1",
      "bb63dcfb203843eeb7421ff11b5228a7",
      "821185cfd61745ce8adc637392bf78f8",
      "1f3ac3b1fe9d41fb93b836f735072703",
      "0a7027ae7f7149bc9067e986c00de3fc",
      "c64987e0fde8482c9cb51b2502218bd9",
      "13758587ac7748cda28690c97cdf77f9",
      "0cde2a34204444fa948f4e2d4b9ba947",
      "1f01fbca169c49b68b10299e26d81523",
      "5d9984af84744606988707c50d4506c9",
      "0e759d7fdb1f4700b28e086f1e34e549",
      "08e37aef2e954c07ab54fbf8bacc7748",
      "199a289278eb40268a4377e4add2a611",
      "fec043c725324aa3abb94bdab744b39e",
      "137b419e6bb44db0bd8fc9b28901a2b4",
      "15ea0120d6744e6dacea1cbcd1a60287",
      "1baed2ed7102478e804bb2ffb29c1985",
      "f9fe7d2e01fa4b419b09e21738a6918f",
      "7d151b58322b47d0ac1b586fc08a24cb",
      "1731fec268d94ca7862f04b14cbf5a39",
      "a52ad630d7fc481b9402440158900289",
      "2a74a36216654bb88b7c0f43289db975",
      "7472cd2381ce45bd8998a4796bc206c2",
      "fa00a82a1eea4d4ab8a7518c68f0b33e",
      "35a51cc1e68c46938e687c39a478b5a3",
      "4bb2d1e64cde4aa3bbe768b32e66c80a",
      "36a9b37ffd05474c970f60142a22f4e5",
      "62a498a5f2e940b49b92589ffebaee0c",
      "5e7580e0d388450e9696c6664f01d87c",
      "4de27b19b17840458dcce998bc235f6f",
      "aec5c9b583364155b4e8e4a75c233b86",
      "1967b3e16a994318882f09a68b5cfd9a",
      "724d70a5ea56415485c194173296b3e0",
      "e177e8e2f26a4e94a6c917459b26621f",
      "a9846333f66f4836a7c5c81ed2e69d45",
      "666682c677e4471f83d187ca70dc4ef8",
      "e534dbbce1d944979abdcffd8e482ec8",
      "5cacc1e1f6da4bacbac7a7bcd084e2d7",
      "ab3617ffd23e40b2a9ce7936c05358ee",
      "14bdf801594d495eb0cb120ef5bddb25",
      "89b6408181354a22a6560b509d1ec870",
      "57ad95190c1e4ded9481a417e1fa2432",
      "47357209fd534059814ed5fc2eeed1fd",
      "e1806ecbe10c46d8b0c94f496d09795a",
      "cbecbef698924d93bd037127e1b159df",
      "41f91e0c26d247d294ce7ca9ddf330d3",
      "0cf331d17cd4407bb73a6fb012b4ba92",
      "ffe40902c949497b88647e826c239475",
      "a49c7f526b3347f4af52519dc9d2074d",
      "7a9095f5a2064292b106e92b3be532c1",
      "569d14f9d172430eba099f90a3b1a3bb",
      "76dfd320620c4ac796d9cd493c6f83c3",
      "55b666492ddd41b3b95043b62084a379",
      "91e82d827fee40a09fd5ce99f5c183af",
      "8fc0c999621242fc9db31cdd3765ac5f",
      "1b3e322741324e6994ba7306b394f815",
      "9557149c87bd4e45b43274a742966395",
      "abd90d562f724faa9456795d93fde170",
      "188bf46324ef4ac487aa6076a7b73c8c",
      "a6267fbeddf944a7be6f1ec9abb0cea2"
     ]
    },
    "id": "7rN6-VsBKwir",
    "outputId": "2a70232d-5c3f-4edd-e9e6-1177f507df4a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b22492ea20e4ff6ac13c70222956830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e1754973524518bb379925783225df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448e57cc2ba84a77a605397c5fd2b51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307eb842339c4fdc94de74509906b632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f3c77757cd4d50b3ef650330381748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7bc871a2524d39a836ab2d4504a65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64987e0fde8482c9cb51b2502218bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1baed2ed7102478e804bb2ffb29c1985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a498a5f2e940b49b92589ffebaee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3617ffd23e40b2a9ce7936c05358ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9095f5a2064292b106e92b3be532c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HuggingFace Embeddings - Instructor embeddings\n",
    "\n",
    "# instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
    "#                                                       model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "\n",
    "embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "model_norm = HuggingFaceBgeEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccP2uFDwfrW4"
   },
   "source": [
    "###mixedbread-ai/mxbai-embed-large-v1\n",
    "\n",
    "note that you have to provide the prompt \"Represent this sentence for searching relevant passages: \"\n",
    "for query if you want to use it for retrieval. Besides that you don't need any prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnAP-cgSfspC"
   },
   "outputs": [],
   "source": [
    "# # loading model\n",
    "# model_name = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "\n",
    "# # Encoding\n",
    "# # encode_kwargs = {'normalized': True}\n",
    "\n",
    "# model_norm = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs={'device': 'cuda'},\n",
    "#     # encode_kwargs=encode_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmj7ETdwBqHW"
   },
   "source": [
    "###Chroma DB\n",
    "\n",
    "Chroma DB is a vector store that is open-source and is utilized for the storage and retrieval of vector embeddings. Its primary purpose is to store embeddings and associated metadata for future use by extensive language models. Furthermore, it can also be employed for semantic search engines that operate on text data.\n",
    "\n",
    "With Chroma DB, you can easily manage text documents, convert text to embeddings, and do similarity searches.\n",
    "\n",
    "https://www.datacamp.com/tutorial/chromadb-tutorial-step-by-step-guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3pbPv_xLn7O"
   },
   "outputs": [],
   "source": [
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'database'\n",
    "\n",
    "# embedding = instructor_embeddings\n",
    "embedding = model_norm\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HciOUGFctmZ"
   },
   "source": [
    "###Retriever\n",
    "\n",
    "vectordb:\n",
    "This appears to be a reference to a module or an object that interacts with a vector database system. Vector databases are specialized storage systems designed to handle high-dimensional vector data efficiently, which is common in machine learning and similar applications where entities are represented as vectors in a high-dimensional space.\n",
    "\n",
    "as_retriever:\n",
    "as_retriever is a method that configures and returns a retriever object. This object is likely used for querying the vector database, particularly for retrieving vectors that are nearest to a given query vector based on some distance metric (e.g., cosine similarity).\n",
    "\n",
    "search_kwargs:\n",
    "The search_kwargs parameter is used to pass additional keyword arguments to the as_retriever method. These arguments are typically used to configure how the search is performed within the vector database.\n",
    "\n",
    "{\"k\": 5}: This dictionary specifies the configuration for the retriever. Here, k is set to 5, which usually means the retriever will return the top 5 closest vectors (or entries) in response to a query. This is often referred to as \"k-nearest neighbors\" search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T0zABxycsNU"
   },
   "outputs": [],
   "source": [
    "# search_kwargs: to return the number of context from the vector stores\n",
    "# also we are having to have a citation to show from which pdf file the info comes from\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb0Sn22Ke6kT"
   },
   "source": [
    "###LangChain\n",
    "LangChain components can be used to build new prompt chains or customize existing templates. it also also includes components that allow LLMs to access new data sets without retraining.\n",
    "\n",
    "LLMs excel at responding to prompts in a general context, but struggle in a specific domain they were never trained on. Prompts are queries people use to seek responses from an LLM. For example, an LLM can provide an answer to how much a computer costs by providing an estimate. However, it can't list the price of a specific computer model that your company sells.\n",
    "\n",
    "To do that, machine learning engineers must integrate the LLM with the organization’s internal data sources and apply prompt engineering*—a practice where a data scientist refines inputs to a generative model with a specific structure and context.\n",
    "\n",
    "LangChain streamlines intermediate steps to develop such data-responsive applications, making prompt engineering more efficient. It is designed to develop diverse applications powered by language models more effortlessly, including chatbots, question-answering, content generation, summarizers, and more.[https://aws.amazon.com/what-is/langchain/]\n",
    "\n",
    "*Prompt engineering is the process where you guide generative artificial intelligence (generative AI) solutions to generate desired outputs.[https://aws.amazon.com/what-is/prompt-engineering/]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S64lItNBe8XP"
   },
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "# You are a drunk, not respectful assistant. Always answer with creativity. Your answers could include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are biased and negative in nature.\"\"\"\n",
    "\n",
    "\n",
    "# Creates a complete prompt\n",
    "def get_prompt(instruction, system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "  sys_prompt = B_SYS + system_prompt + E_SYS\n",
    "  prompt_template = B_INST + sys_prompt + instruction + E_INST\n",
    "  # print(prompt_template)\n",
    "  return prompt_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tydXizj7PcA"
   },
   "source": [
    "###Building a new system propmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgljIHcLUQtl"
   },
   "outputs": [],
   "source": [
    "# instruction = \"Summarize the following text for me {text}\"\n",
    "\n",
    "# system_propmt = \"Your are an expert in text and article summarization and reducing the number of words. All the sentences and the grammar should be academically enhanced by you.\"\n",
    "\n",
    "# get_prompt(inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIM0MAZnABE7"
   },
   "source": [
    "###Building new system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6Yv2nyI8Wdk"
   },
   "outputs": [],
   "source": [
    "# system_prompt = \"You are an expert assistant in translation.\"\n",
    "# instruction = \"Convert the text from English to Italian:\\n\\n {text}\"\n",
    "# prompt_template = get_prompt(instruction, system_prompt)\n",
    "# print(prompt_template)\n",
    "\n",
    "# prompt = PromptTemplate(template=prompt_template, input_variable=[\"text\"])\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rObFDsZFgt-x"
   },
   "source": [
    "### Completely different system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f15ov0aYgsmx"
   },
   "outputs": [],
   "source": [
    "# diffrent system propmt\n",
    "system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "#mxbai syetem prompt\n",
    "# system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "# instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "# Question: Represent this sentence for searching relevant passages: {question}\"\"\"\n",
    "# get_prompt(instruction, sys_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKlDR9wznrAs"
   },
   "source": [
    "### RetrievalQA\n",
    "\n",
    "Chain Type\n",
    "\n",
    "The default chain_type=\"stuff\" uses ALL of the text from the documents in the prompt. It actually doesn’t work with our example because it exceeds the token limit and causes rate-limiting errors. That’s why in this example, we had to use other chain types for example \"map_reduce\". What are the other chain types?\n",
    "\n",
    "map_reduce: It separates texts into batches (as an example, you can define batch size in llm=OpenAI(batch_size=5)), feeds each batch with the question to LLM separately, and comes up with the final answer based on the answers from each batch.\n",
    "\n",
    "refine : It separates texts into batches, feeds the first batch to LLM, and feeds the answer and the second batch to LLM. It refines the answer by going through all the batches.\n",
    "\n",
    "map-rerank: It separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch.\n",
    "\n",
    "https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4bvRKJmnVaC"
   },
   "outputs": [],
   "source": [
    "# Create the template prompt\n",
    "prompt_template = get_prompt(instruction, system_prompt)\n",
    "llm_prompt = PromptTemplate(\n",
    "    template = prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "chain_type_kwargs = {\"prompt\": llm_prompt}\n",
    "\n",
    "# To create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = hf_llm,\n",
    "    chain_type = \"stuff\", #  uses ALL of the text from the documents in the prompt\n",
    "    retriever = retriever,\n",
    "    chain_type_kwargs = chain_type_kwargs,\n",
    "    return_source_documents = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVmX_XdRWsdb"
   },
   "source": [
    "###Format the generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD8aIxbRrnuL"
   },
   "outputs": [],
   "source": [
    "# to format the response and cite sources\n",
    "# textwrap: Used to wrap or fill text into a specified width. This is helpful for formatting output text to make it more readable\n",
    "\n",
    "# To trim a given string (text) at the point where a specific substring (prompt) first appears\n",
    "def trim_text(output_text, search_phrase):\n",
    "  phrase = search_phrase\n",
    "  index = output_text.find(phrase)\n",
    "  if index != -1:\n",
    "    return output_text[index:] # Trim everything from the start of text up to the phrase/symbol\n",
    "  else:\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# Removes occurrences of a substring from a string, typically used here to clean up the generated text by removing predefined markers or prompts\n",
    "def remove_substring(output, substring):\n",
    "  return output.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def wrap_text(text, width=150):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_generated_response(generated_response):\n",
    "    # source_list = []\n",
    "    # print(wrap_text(generated_response['result']))\n",
    "    wrapped_response = wrap_text(generated_response['result'])\n",
    "    final_response = trim_text(wrapped_response, '[/INST]')\n",
    "    final_response = remove_substring(final_response, '[/INST]')\n",
    "    print(final_response)\n",
    "\n",
    "    print('\\n\\nSources:')\n",
    "    for source in generated_response[\"source_documents\"]:\n",
    "      # source_list.append(source.metadata['source'])\n",
    "      print(source.metadata['source'])\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecoGtb-6vMqE"
   },
   "outputs": [],
   "source": [
    "# qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6hX38uuMBat"
   },
   "outputs": [],
   "source": [
    "# For retrieval we need to pass this prompt.\n",
    "# query = 'Represent this sentence for searching relevant passages: A man is eating a piece of bread'\n",
    "\n",
    "# query = \"What are Large Language Models?\"\n",
    "# response = qa_chain(query)\n",
    "# # process_generated_response(response)\n",
    "# res_, sl = process_generated_response(response)\n",
    "# print(f\"\\n\\n\\n res: {res_}\\n\\n\\n sl: {sl}\")\n",
    "# for _ in sl:\n",
    "#   print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c6V0xzpMFkL"
   },
   "outputs": [],
   "source": [
    "# query = \"How many tokens was LLaMA-2 trained on?\"\n",
    "# response = qa_chain(query)\n",
    "# res_, sl = process_generated_response(response)\n",
    "# print(f\"\\n\\n\\n res: {res_}\\n\\n\\n sl: {sl}\")\n",
    "# for _ in sl:\n",
    "#   print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m_TW256un6W"
   },
   "outputs": [],
   "source": [
    "# # For retrieval we need to pass this prompt.\n",
    "# # query = 'Represent this sentence for searching relevant passages: A man is eating a piece of bread'\n",
    "\n",
    "# query = \"What are Large Language Models?\"\n",
    "# response = qa_chain(query)\n",
    "# process_generated_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfUWg3Iv4OjA",
    "outputId": "5cf71ad3-220f-4e32-b3a9-fa94be06e640"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaMA-2 was trained on 2 trillion tokens.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      " LLaMA-2 was trained on 2 trillion tokens.\n"
     ]
    }
   ],
   "source": [
    "query = \"How many tokens was LLaMA-2 trained on?\"\n",
    "response = qa_chain(query)\n",
    "print(process_generated_response(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRfwvXgu2wo6"
   },
   "source": [
    "###retrieving questions and generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq3NQsho20F_"
   },
   "outputs": [],
   "source": [
    "# %pip -q install dropbox\n",
    "\n",
    "# import pathlib\n",
    "# import pandas as pd\n",
    "# import dropbox\n",
    "# from dropbox.exceptions import AuthError\n",
    "\n",
    "# DROPBOX_ACCESS_TOKEN = ''\n",
    "\n",
    "# # Connect to the Dropbox API\n",
    "# def dropbox_connect():\n",
    "#   try:\n",
    "#     dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)\n",
    "#   except AuthError as e:\n",
    "#     print(f\"Error connecting to Dropbox with access token: {str(e)}\" )\n",
    "#   return dbx\n",
    "\n",
    "\n",
    "# # Download the file\n",
    "# def dropbox_download(dbx_file_path, local_file_path):\n",
    "#   try:\n",
    "#     dbx = dropbox_connect()\n",
    "\n",
    "#     with open(local_file_path, 'wt') as f:\n",
    "#       metadata, result = dbx.files_download(path=dbx_file_path)\n",
    "#       f.write(result.content)\n",
    "#   except Exception as e:\n",
    "#       print(f\"Error downloading file from dropbox: {str(e)}\")\n",
    "\n",
    "# dbx_path_file = 'All files/Apps/LLMs-RAG/Questions.csv'\n",
    "# local_path_file = '/content/Questions'\n",
    "# Questions = dropbox_download(dbx_path_file, local_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwLYCDl7jnw5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_question_dict(questions_file_path):\n",
    "  qfile_path = questions_file_path\n",
    "  dfQ = pd.read_csv(qfile_path)\n",
    "  # dfQ\n",
    "  qlist = [dfQ.loc[i, 'Question'] for i in range(len(dfQ)) ]\n",
    "  # print(\"These are the General Questions: \\n\")\n",
    "  # # print(f\"{dfQ.loc[:, 'Question']}\")\n",
    "  # for index in range(len(dfQ)):\n",
    "  #   print(f\"Q {index+1}: {dfQ.loc[index,'Question']}\")\n",
    "  qa_dict = {key: None for key in qlist}\n",
    "\n",
    "  return qa_dict\n",
    "\n",
    "\n",
    "def generate_qa_dict(question_dict):\n",
    "  qdict = question_dict\n",
    "  for k in qdict.keys():\n",
    "    # print(str(k))\n",
    "    query = str(k)\n",
    "    response = qa_chain(query)\n",
    "    final_res = process_generated_response(response)\n",
    "    qdict.update({k : final_res})\n",
    "\n",
    "    return qdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UnkzDSFPkYh"
   },
   "source": [
    "# Download the Questions in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfcSjCQ_PuMb",
    "outputId": "6ed67bf3-7b70-4300-8e90-a82326b1c05d"
   },
   "outputs": [],
   "source": [
    "!wget -O Questionscsv.zip your_path_to_the_zip_file\n",
    "!unzip -q Questionscsv.zip -d questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JqYaxt8BMO4"
   },
   "source": [
    "###General QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPCcoSyC4XLd",
    "outputId": "5fe3346d-cf11-4b8a-83eb-636c1d1f61cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Newton's first law of motion, also known as the law of inertia, states that an object will\n",
      "remain at rest or continue moving at a constant velocity in a straight line unless acted upon by a net external force.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The force of gravity between two objects decreases with\n",
      "increasing distance, following an inverse square law. This means that the force is proportional to the product of the masses of the two objects and\n",
      "inversely proportional to the square of the distance between them.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The principle of conservation of energy states that energy cannot be created or\n",
      "destroyed in an isolated system. It is a fundamental law of physics that applies to all forms of energy, including potential energy, kinetic energy,\n",
      "thermal energy, electrical energy, and radiation energy. This principle is a direct consequence of the general covariance of the theory of relativity.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A prism separates white light into its component colors based on\n",
      "the refractive index of each wavelength. As light enters the prism, it slows down and bends at different angles depending on its wavelength. Shorter\n",
      "wavelengths (blue and violet light) bend more than longer wavelengths (red and orange light), causing them to spread out further when exiting the\n",
      "prism, resulting in a rainbow-like spectrum.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum physics is a branch of physics that describes natural phenomena at the subatomic level. It is one of\n",
      "the most accurate theories ever developed to explain our universe and is the foundation for advanced technologies like quantum computing and\n",
      "communication. Despite some ongoing debates about its foundations, the international community continues to work towards shedding light on its\n",
      "physical meaning and pushing the boundaries of its application.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A neural network is a type of machine learning model inspired by the human brain, consisting of\n",
      "interconnected nodes or neurons that process and transmit information using weights and biases. Neural networks come in various architectures,\n",
      "including feed-forward networks, recurrent neural networks (RNN), convolutional neural networks (CNN), and transformer models, among others. They are\n",
      "trained using large amounts of labeled data to identify patterns and make predictions based on new inputs.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Activation functions serve the purpose of introducing nonlinearity in\n",
      "neural networks, allowing them to learn complex patterns and relationships in data. They introduce nonlinearity by applying an nonlinear function to\n",
      "the weighted sum of inputs, outputting a new value that can then be passed through the next layer for further processing.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Supervised learning involves learning from\n",
      "labeled data, where each example includes an input and its corresponding output label. The goal is to learn a mapping from inputs to outputs based on\n",
      "these examples. Unsupervised learning, on the other hand, deals with unlabeled data, meaning there is no explicit output label provided. Instead, the\n",
      "algorithm looks for patterns or structures within the data itself. Common techniques include clustering, dimensionality reduction, and density\n",
      "estimation.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overfitting refers to a neural network that\n",
      "learns the training data too well, including its noise and irrelevant details, resulting in poor performance on new, unseen data. This can lead to\n",
      "models that memorize the training data rather than generalizing to new inputs. Techniques like dropout and regularization are used to prevent\n",
      "overfitting by adding randomness during training and reducing the complexity of the model.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      " The learning rate is a parameter in optimization\n",
      "algorithms used to update the weights in neural networks during training. It determines the size of the weight updates at each iteration, influencing\n",
      "how quickly the model converges towards the optimal solution. A high learning rate may cause the model to overshoot the minimum, while a low learning\n",
      "rate may lead to slow convergence. Adjusting the learning rate appropriately helps ensure effective training and good model performance.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n"
     ]
    }
   ],
   "source": [
    "# General Questions\n",
    "\n",
    "questions_file_path = '/content/questions/Questions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka9sG8ZUMnOC",
    "outputId": "97e681e5-6380-47fb-9001-96b18d1bbf5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What is Newton's first law of motion?\n",
      " response:  Newton's first law of motion, also known as the law of inertia, states that an object will\n",
      "remain at rest or continue moving at a constant velocity in a straight line unless acted upon by a net external force.\n",
      "\n",
      "\n",
      "Q1: How does the force of gravity between two objects change with distance?\n",
      " response:  The force of gravity between two objects decreases with\n",
      "increasing distance, following an inverse square law. This means that the force is proportional to the product of the masses of the two objects and\n",
      "inversely proportional to the square of the distance between them.\n",
      "\n",
      "\n",
      "Q2: What is the principle of conservation of energy?\n",
      " response:  The principle of conservation of energy states that energy cannot be created or\n",
      "destroyed in an isolated system. It is a fundamental law of physics that applies to all forms of energy, including potential energy, kinetic energy,\n",
      "thermal energy, electrical energy, and radiation energy. This principle is a direct consequence of the general covariance of the theory of relativity.\n",
      "\n",
      "\n",
      "Q3: Explain how a prism splits white light into a spectrum of colors.\n",
      " response:  A prism separates white light into its component colors based on\n",
      "the refractive index of each wavelength. As light enters the prism, it slows down and bends at different angles depending on its wavelength. Shorter\n",
      "wavelengths (blue and violet light) bend more than longer wavelengths (red and orange light), causing them to spread out further when exiting the\n",
      "prism, resulting in a rainbow-like spectrum.\n",
      "\n",
      "\n",
      "Q4: What is quantum physics?\n",
      " response:  Quantum physics is a branch of physics that describes natural phenomena at the subatomic level. It is one of\n",
      "the most accurate theories ever developed to explain our universe and is the foundation for advanced technologies like quantum computing and\n",
      "communication. Despite some ongoing debates about its foundations, the international community continues to work towards shedding light on its\n",
      "physical meaning and pushing the boundaries of its application.\n",
      "\n",
      "\n",
      "Q5: What is a neural network?\n",
      " response:  A neural network is a type of machine learning model inspired by the human brain, consisting of\n",
      "interconnected nodes or neurons that process and transmit information using weights and biases. Neural networks come in various architectures,\n",
      "including feed-forward networks, recurrent neural networks (RNN), convolutional neural networks (CNN), and transformer models, among others. They are\n",
      "trained using large amounts of labeled data to identify patterns and make predictions based on new inputs.\n",
      "\n",
      "\n",
      "Q6: What function do activation functions serve in neural networks?\n",
      " response:  Activation functions serve the purpose of introducing nonlinearity in\n",
      "neural networks, allowing them to learn complex patterns and relationships in data. They introduce nonlinearity by applying an nonlinear function to\n",
      "the weighted sum of inputs, outputting a new value that can then be passed through the next layer for further processing.\n",
      "\n",
      "\n",
      "Q7: What is the difference between supervised and unsupervised learning in machine learning?\n",
      " response:  Supervised learning involves learning from\n",
      "labeled data, where each example includes an input and its corresponding output label. The goal is to learn a mapping from inputs to outputs based on\n",
      "these examples. Unsupervised learning, on the other hand, deals with unlabeled data, meaning there is no explicit output label provided. Instead, the\n",
      "algorithm looks for patterns or structures within the data itself. Common techniques include clustering, dimensionality reduction, and density\n",
      "estimation.\n",
      "\n",
      "\n",
      "Q8: Can you explain what overfitting means in the context of training a neural network?\n",
      " response:  Overfitting refers to a neural network that\n",
      "learns the training data too well, including its noise and irrelevant details, resulting in poor performance on new, unseen data. This can lead to\n",
      "models that memorize the training data rather than generalizing to new inputs. Techniques like dropout and regularization are used to prevent\n",
      "overfitting by adding randomness during training and reducing the complexity of the model.\n",
      "\n",
      "\n",
      "Q9: What is the significance of the learning rate in training neural networks?\n",
      " response:  The learning rate is a parameter in optimization\n",
      "algorithms used to update the weights in neural networks during training. It determines the size of the weight updates at each iteration, influencing\n",
      "how quickly the model converges towards the optimal solution. A high learning rate may cause the model to overshoot the minimum, while a low learning\n",
      "rate may lead to slow convergence. Adjusting the learning rate appropriately helps ensure effective training and good model performance.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFcTjzUxM-Tp"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'generalQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH4yrXbENATr"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'generalQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK87KAbgBPs4"
   },
   "source": [
    "###Astro Cosmology QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxE-7r7u7vTM",
    "outputId": "fc713f07-5de5-4c52-9377-a0982b5b6376"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The goal of studying astrophysics and cosmology is to understand the\n",
      "physical properties and evolution of the universe, including stars, galaxies, and large-scale structures, as well as the origins of the universe\n",
      "itself. This field combines observations and theories to explore topics such as the nature of dark matter and dark energy, the expansion of the\n",
      "universe, and the search for extraterrestrial life.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Big Bang theory is supported by various lines of evidence including the cosmic microwave\n",
      "background radiation discovered by Penzias and Wilson, the abundance of light elements, the large scale structure of the universe, and the redshift of\n",
      "distant galaxies among others. However, it's important to note that scientific theories are never proven to be absolutely true, but rather are\n",
      "accepted based on the available evidence and the ability to make accurate predictions. The Big Bang theory continues to be the favored explanation for\n",
      "the origins of the universe due to its explanatory power and consistency with observational data.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inflationary cosmology is important because it explains the large-scale homogeneity and\n",
      "isotropy of the observable universe through the exponential expansion of space-time during the inflationary epoch. This helps resolve issues related\n",
      "to the flatness and horizon problems in cosmology.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The precision era in cosmology is marked by the interpretation of Hubble's redshift-\n",
      "distance cosmology and large-scale structure into a truly quantitative science where theory and observations can progress side by side. Two catalogs\n",
      "taking data at the moment and revolutionizing the field are the 2-degree-Field (2dF)Catalog and the Sloan Digital Sky Survey (SDSS).\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cosmic microwave background indicates the presence of residual thermal\n",
      "radiation left over from the Big Bang event. Its discovery supported the Big Bang theory as an explanation for the origins of the universe.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current challenges in cosmology include the fact that modern cosmological models, which are\n",
      "modified Friedmann models, are unsatisfactory. Only 0.5% of the total matter-energy budget of the universe has been proven to exist from direct\n",
      "observations. The desired outcome of determining the matter-energy content of the universe does not conform to the real world. Additionally, there are\n",
      "ongoing developments in inflationary cosmology as the paradigm for the origin of the global structure of the universe and the origins of density\n",
      "perturbations responsible for structure in our local patch.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in cosmology include the invention of the inflationary\n",
      "theory by Alan Guth in the early 1980s on the theoretical side, and the first results from the Cosmic Background Explorer satellite being published in\n",
      "the early 1990s on the observational side. These discoveries strengthened the expanding universe paradigm. Additionally, advancements in multi-object\n",
      "fiber spectroscopy, such as the construction of the 2-degree-Field (2dF) Spectrograph and the Sloan Digital Sky Survey (SDSS), have revolutionized the\n",
      "field by allowing for a larger yield of redshifts to be collected.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The observational revolution in cosmology refers to the significant\n",
      "advancements in our understanding of the universe through precise observations, leading to accurate determination of cosmological parameters such as\n",
      "the rate of expansion, matter content, cosmological constant, spatial curvature, and age of the universe. These discoveries provide valuable insights\n",
      "into various aspects of cosmology, including the evolution of the universe, the nature of dark matter and dark energy, and the origin of structure in\n",
      "the universe. This revolution has made cosmology a phenomenological science and will continue to dominate research in the new millennium.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modern cosmological observations are explained primarily by the standard\n",
      "Big Bang theory, which has been incorporated into the larger picture of cosmological inflation, forming the new standard cosmological model. These\n",
      "theories provide explanations for various phenomena including the redshift-distance relation, the distribution of galaxies and their evolution, and\n",
      "the large-scale structure measurements. However, it's important to remember that all physical theories are approximations of reality and may fail if\n",
      "pushed too far. The standard Big Bang theory is supported by a wealth of evidence but continues to evolve within the broader context of cosmological\n",
      "inflation. Other theories like plasma-redshift cosmology propose alternative explanations for certain observations, but they remain subject to ongoing\n",
      "research and debate.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      " The future of cosmology holds the promise of making it a truly quantitative science where\n",
      "theory and observations can progress side by side. With ongoing experiments providing important results almost every month, we are entering a\n",
      "precision era in cosmology. Many observables will soon be measured with a few percent accuracy, making cosmology a phenomenological science. It is\n",
      "important to remember that all physical theories are approximations of reality that can fail if pushed too far. The standard Big Bang theory, while\n",
      "supported by a wealth of evidence, has been incorporated into the larger picture of cosmological inflation, which has become the new standard\n",
      "cosmological model. All cosmological issues are now formulated within the context of the inflationary paradigm.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n"
     ]
    }
   ],
   "source": [
    "# Astro Cosmology Questions\n",
    "\n",
    "questions_file_path = '/content/questions/AstroCosmoQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zvx-XOo9AaGW",
    "outputId": "196b4dad-5b6d-4131-f9ae-f681c7a284ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What is the goal of studying astrophysics and cosmology?\n",
      " response:  The goal of studying astrophysics and cosmology is to understand the\n",
      "physical properties and evolution of the universe, including stars, galaxies, and large-scale structures, as well as the origins of the universe\n",
      "itself. This field combines observations and theories to explore topics such as the nature of dark matter and dark energy, the expansion of the\n",
      "universe, and the search for extraterrestrial life.\n",
      "\n",
      "\n",
      "Q1: What supports the Big Bang theory?\n",
      " response:  The Big Bang theory is supported by various lines of evidence including the cosmic microwave\n",
      "background radiation discovered by Penzias and Wilson, the abundance of light elements, the large scale structure of the universe, and the redshift of\n",
      "distant galaxies among others. However, it's important to note that scientific theories are never proven to be absolutely true, but rather are\n",
      "accepted based on the available evidence and the ability to make accurate predictions. The Big Bang theory continues to be the favored explanation for\n",
      "the origins of the universe due to its explanatory power and consistency with observational data.\n",
      "\n",
      "\n",
      "Q2: Why is inflationary cosmology important?\n",
      " response:  Inflationary cosmology is important because it explains the large-scale homogeneity and\n",
      "isotropy of the observable universe through the exponential expansion of space-time during the inflationary epoch. This helps resolve issues related\n",
      "to the flatness and horizon problems in cosmology.\n",
      "\n",
      "\n",
      "Q3: What marks the precision era in cosmology?\n",
      " response:  The precision era in cosmology is marked by the interpretation of Hubble's redshift-\n",
      "distance cosmology and large-scale structure into a truly quantitative science where theory and observations can progress side by side. Two catalogs\n",
      "taking data at the moment and revolutionizing the field are the 2-degree-Field (2dF)Catalog and the Sloan Digital Sky Survey (SDSS).\n",
      "\n",
      "\n",
      "Q4: What does the cosmic microwave background indicate?\n",
      " response:  The cosmic microwave background indicates the presence of residual thermal\n",
      "radiation left over from the Big Bang event. Its discovery supported the Big Bang theory as an explanation for the origins of the universe.\n",
      "\n",
      "\n",
      "Q5: What are current challenges in cosmology?\n",
      " response:  Current challenges in cosmology include the fact that modern cosmological models, which are\n",
      "modified Friedmann models, are unsatisfactory. Only 0.5% of the total matter-energy budget of the universe has been proven to exist from direct\n",
      "observations. The desired outcome of determining the matter-energy content of the universe does not conform to the real world. Additionally, there are\n",
      "ongoing developments in inflationary cosmology as the paradigm for the origin of the global structure of the universe and the origins of density\n",
      "perturbations responsible for structure in our local patch.\n",
      "\n",
      "\n",
      "Q6: What recent developments have occurred in cosmology?\n",
      " response:  Recent developments in cosmology include the invention of the inflationary\n",
      "theory by Alan Guth in the early 1980s on the theoretical side, and the first results from the Cosmic Background Explorer satellite being published in\n",
      "the early 1990s on the observational side. These discoveries strengthened the expanding universe paradigm. Additionally, advancements in multi-object\n",
      "fiber spectroscopy, such as the construction of the 2-degree-Field (2dF) Spectrograph and the Sloan Digital Sky Survey (SDSS), have revolutionized the\n",
      "field by allowing for a larger yield of redshifts to be collected.\n",
      "\n",
      "\n",
      "Q7: What does the observational revolution in cosmology entail?\n",
      " response:  The observational revolution in cosmology refers to the significant\n",
      "advancements in our understanding of the universe through precise observations, leading to accurate determination of cosmological parameters such as\n",
      "the rate of expansion, matter content, cosmological constant, spatial curvature, and age of the universe. These discoveries provide valuable insights\n",
      "into various aspects of cosmology, including the evolution of the universe, the nature of dark matter and dark energy, and the origin of structure in\n",
      "the universe. This revolution has made cosmology a phenomenological science and will continue to dominate research in the new millennium.\n",
      "\n",
      "\n",
      "Q8: What theoretical models explain cosmological observations?\n",
      " response:  Modern cosmological observations are explained primarily by the standard\n",
      "Big Bang theory, which has been incorporated into the larger picture of cosmological inflation, forming the new standard cosmological model. These\n",
      "theories provide explanations for various phenomena including the redshift-distance relation, the distribution of galaxies and their evolution, and\n",
      "the large-scale structure measurements. However, it's important to remember that all physical theories are approximations of reality and may fail if\n",
      "pushed too far. The standard Big Bang theory is supported by a wealth of evidence but continues to evolve within the broader context of cosmological\n",
      "inflation. Other theories like plasma-redshift cosmology propose alternative explanations for certain observations, but they remain subject to ongoing\n",
      "research and debate.\n",
      "\n",
      "\n",
      "Q9: What does the future hold for cosmology?\n",
      " response:  The future of cosmology holds the promise of making it a truly quantitative science where\n",
      "theory and observations can progress side by side. With ongoing experiments providing important results almost every month, we are entering a\n",
      "precision era in cosmology. Many observables will soon be measured with a few percent accuracy, making cosmology a phenomenological science. It is\n",
      "important to remember that all physical theories are approximations of reality that can fail if pushed too far. The standard Big Bang theory, while\n",
      "supported by a wealth of evidence, has been incorporated into the larger picture of cosmological inflation, which has become the new standard\n",
      "cosmological model. All cosmological issues are now formulated within the context of the inflationary paradigm.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3leHbYjAaRT"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'AstroCosmoQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJP_s-xMAabb"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'AstroCosmoQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABHzTWuRBT6R"
   },
   "source": [
    "###Astro Physics QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mL5xk1MAao-",
    "outputId": "7045a74c-1be4-4f3a-e5b7-88c2984aac1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Chandrasekhar limit is a theoretical maximum mass for a\n",
      "stable white dwarf star, beyond which it cannot sustain its own weight due to the degenerate pressure of its electrons. It is approximately equal to\n",
      "1.43 solar masses. In astrophysics, it marks the boundary between white dwarfs and more massive objects like neutron stars or black holes.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " White dwarf stars are dense remnants of low-mass stars\n",
      "that have completed their nuclear fuel burning and ejected their outer layers. They provide important insights into the later stages of stellar\n",
      "evolution by demonstrating that the pressure of electron degeneracy can support a star against its own gravity, allowing it to maintain a stable\n",
      "configuration despite its high density. This discovery helped resolve the mystery of how white dwarfs could exist given their extreme compactness and\n",
      "the inability of normal gas pressure to support them against gravity. It also led to the realization that the maximum mass of a stable white dwarf is\n",
      "determined by the Chandrasekhar limit, which marks the boundary between white dwarfs and more massive objects that collapse into other types of\n",
      "compact stars like neutron stars or black holes.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Experimental techniques used to observe and study black holes\n",
      "include studying the deflection of light around them, analyzing the X-ray emissions from accretion disks surrounding them, and observing gravitational\n",
      "waves produced by merging black holes. Modern techniques and instrumentations used include advanced telescopes, interferometers, and spacecraft. For\n",
      "example, the Event Horizon Telescope project uses a network of telescopes around the world to create virtual Earth-sized telescopes and image the\n",
      "event horizons of black holes. Additionally, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and Virgo detectors have detected\n",
      "gravitational waves produced by merging black holes.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arthur Eddington played a\n",
      "significant role in validating General Relativity through astronomical observations. He became an expert in the theory after being introduced to it by\n",
      "de Sitter, and quickly became Einstein's self-appointed evangelist in Britain. Despite his belief in the theory, Eddington organized an expedition to\n",
      "observe the 1919 solar eclipse to provide observational evidence for the gravitational deflection of light predicted by General Relativity. This\n",
      "observation helped to further establish the theory's credibility.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The discovery of degenerate\n",
      "matter, specifically degenerate electron pressure in white dwarfs and degenerate electron and neutron pressure in neutron stars, explained how these\n",
      "compact objects could achieve equilibrium despite their immense gravitational forces. This understanding came from the application of quantum\n",
      "mechanics and the Pauli exclusion principle, which prohibits identical fermions from occupying the same quantum state. As a result, the fermions in\n",
      "these objects exert a high pressure opposing the gravitational collapse, allowing them to maintain their stable, compact structures.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eddington's opposition to\n",
      "Chandrasekhar's theories on stellar evolution may have significantly delayed the progress of research in this field for over 20 years. This is\n",
      "according to some authors who claim that Eddington's \"scientific prejudices\" and attempts to suppress new scientific ideas, particularly those related\n",
      "to quantum mechanics and special relativity theory, hindered the acceptance and further exploration of Chandrasekhar's discoveries.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I cannot directly answer your question using the context text provided, as the text does not mention any experimental advances related to the direct\n",
      "observation of gravitational waves or their significance for astrophysics. However, I can provide some background information. Gravitational waves\n",
      "were first predicted by Albert Einstein's theory of general relativity in 1915. They are ripples in the fabric of spacetime caused by accelerating\n",
      "masses, such as those produced during mergers of black holes or neutron stars. The Laser Interferometer Gravitational-Wave Observatory (LIGO) and\n",
      "Virgo collaborations detected the first confirmed gravitational wave signals in 2015 and 2016, respectively. These detections marked a major milestone\n",
      "in astrophysics and opened a new window into understanding the universe through gravitational wave observations.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The discovery of black holes\n",
      "challenges our understanding of physics under extreme conditions by presenting us with a mathematical singularity, which is currently identified with\n",
      "a black hole. However, the existence of black holes as physical objects is still debated among scientists. Some argue that they do not exist and that\n",
      "the Schwarzschild singularity should be associated with another astronomical object instead. This debate highlights the ongoing efforts to understand\n",
      "the behavior of matter and energy under extreme conditions predicted by general relativity.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The contemporary\n",
      "challenges in theoretical physics suggested by the study of cosmological phenomena include solving initial condition problems, such as the origin of\n",
      "the universe and the matter-energy budget, which cannot be answered by the standard Big Bang theory alone. These issues may be resolved or explained\n",
      "by new physical principles at higher energies in the early universe. Additionally, there are many unresolved issues related to these problems, but\n",
      "there might be reasons to be optimistic in the near future.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/ASTROPHYSICS AND COSMOLOGY.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      "/content/papers/Selected papers-astrophysics-physics.pdf\n",
      " Based on the context provided, the\n",
      "article discusses the potential application of Augmented Reality (AR) technology in advancing our exploration of cosmological phenomena. It mentions\n",
      "that AR is becoming a mature technology that can be easily used and implemented by non-experts, leading to an increase in the number of AR tools and\n",
      "apps. Additionally, the article suggests that AR could potentially offer new ways of communicating scientific results and data through social media\n",
      "and publications. However, the article also acknowledges some limitations of AR in astrophysical research, including complexity, accessibility, and\n",
      "long-term stability. Therefore, while the article does not specifically mention other future technologies, it emphasizes the importance and potential\n",
      "of AR in advancing our understanding of cosmological phenomena.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n"
     ]
    }
   ],
   "source": [
    "# Astro Cosmology Questions\n",
    "\n",
    "questions_file_path = '/content/questions/AstroPhysicsQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-a3kjJnWAas7",
    "outputId": "5f0cb345-dfbb-4af6-dc5b-13fa36053275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What is the Chandrasekhar limit and its significance in astrophysics?\n",
      " response:  The Chandrasekhar limit is a theoretical maximum mass for a\n",
      "stable white dwarf star, beyond which it cannot sustain its own weight due to the degenerate pressure of its electrons. It is approximately equal to\n",
      "1.43 solar masses. In astrophysics, it marks the boundary between white dwarfs and more massive objects like neutron stars or black holes.\n",
      "\n",
      "\n",
      "Q1: How do white dwarf stars contribute to our understanding of stellar evolution?\n",
      " response:  White dwarf stars are dense remnants of low-mass stars\n",
      "that have completed their nuclear fuel burning and ejected their outer layers. They provide important insights into the later stages of stellar\n",
      "evolution by demonstrating that the pressure of electron degeneracy can support a star against its own gravity, allowing it to maintain a stable\n",
      "configuration despite its high density. This discovery helped resolve the mystery of how white dwarfs could exist given their extreme compactness and\n",
      "the inability of normal gas pressure to support them against gravity. It also led to the realization that the maximum mass of a stable white dwarf is\n",
      "determined by the Chandrasekhar limit, which marks the boundary between white dwarfs and more massive objects that collapse into other types of\n",
      "compact stars like neutron stars or black holes.\n",
      "\n",
      "\n",
      "Q2: What experimental techniques are used to observe and study black holes?\n",
      " response:  Experimental techniques used to observe and study black holes\n",
      "include studying the deflection of light around them, analyzing the X-ray emissions from accretion disks surrounding them, and observing gravitational\n",
      "waves produced by merging black holes. Modern techniques and instrumentations used include advanced telescopes, interferometers, and spacecraft. For\n",
      "example, the Event Horizon Telescope project uses a network of telescopes around the world to create virtual Earth-sized telescopes and image the\n",
      "event horizons of black holes. Additionally, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and Virgo detectors have detected\n",
      "gravitational waves produced by merging black holes.\n",
      "\n",
      "\n",
      "Q3: What role did Arthur Eddington play in validating General Relativity through astronomical observations?\n",
      " response:  Arthur Eddington played a\n",
      "significant role in validating General Relativity through astronomical observations. He became an expert in the theory after being introduced to it by\n",
      "de Sitter, and quickly became Einstein's self-appointed evangelist in Britain. Despite his belief in the theory, Eddington organized an expedition to\n",
      "observe the 1919 solar eclipse to provide observational evidence for the gravitational deflection of light predicted by General Relativity. This\n",
      "observation helped to further establish the theory's credibility.\n",
      "\n",
      "\n",
      "Q4: How has the concept of degenerate matter advanced our knowledge of compact objects like neutron stars?\n",
      " response:  The discovery of degenerate\n",
      "matter, specifically degenerate electron pressure in white dwarfs and degenerate electron and neutron pressure in neutron stars, explained how these\n",
      "compact objects could achieve equilibrium despite their immense gravitational forces. This understanding came from the application of quantum\n",
      "mechanics and the Pauli exclusion principle, which prohibits identical fermions from occupying the same quantum state. As a result, the fermions in\n",
      "these objects exert a high pressure opposing the gravitational collapse, allowing them to maintain their stable, compact structures.\n",
      "\n",
      "\n",
      "Q5: What are the implications of Eddington's opposition to Chandrasekhar's theories on stellar evolution?\n",
      " response:  Eddington's opposition to\n",
      "Chandrasekhar's theories on stellar evolution may have significantly delayed the progress of research in this field for over 20 years. This is\n",
      "according to some authors who claim that Eddington's \"scientific prejudices\" and attempts to suppress new scientific ideas, particularly those related\n",
      "to quantum mechanics and special relativity theory, hindered the acceptance and further exploration of Chandrasekhar's discoveries.\n",
      "\n",
      "\n",
      "Q6: What experimental advances have allowed for the direct observation of gravitational waves, and what do they signify for astrophysics?\n",
      " response: \n",
      "I cannot directly answer your question using the context text provided, as the text does not mention any experimental advances related to the direct\n",
      "observation of gravitational waves or their significance for astrophysics. However, I can provide some background information. Gravitational waves\n",
      "were first predicted by Albert Einstein's theory of general relativity in 1915. They are ripples in the fabric of spacetime caused by accelerating\n",
      "masses, such as those produced during mergers of black holes or neutron stars. The Laser Interferometer Gravitational-Wave Observatory (LIGO) and\n",
      "Virgo collaborations detected the first confirmed gravitational wave signals in 2015 and 2016, respectively. These detections marked a major milestone\n",
      "in astrophysics and opened a new window into understanding the universe through gravitational wave observations.\n",
      "\n",
      "\n",
      "Q7: How does the discovery of black holes challenge our understanding of physics under extreme conditions?\n",
      " response:  The discovery of black holes\n",
      "challenges our understanding of physics under extreme conditions by presenting us with a mathematical singularity, which is currently identified with\n",
      "a black hole. However, the existence of black holes as physical objects is still debated among scientists. Some argue that they do not exist and that\n",
      "the Schwarzschild singularity should be associated with another astronomical object instead. This debate highlights the ongoing efforts to understand\n",
      "the behavior of matter and energy under extreme conditions predicted by general relativity.\n",
      "\n",
      "\n",
      "Q8: What are the contemporary challenges in theoretical physics suggested by the study of cosmological phenomena?\n",
      " response:  The contemporary\n",
      "challenges in theoretical physics suggested by the study of cosmological phenomena include solving initial condition problems, such as the origin of\n",
      "the universe and the matter-energy budget, which cannot be answered by the standard Big Bang theory alone. These issues may be resolved or explained\n",
      "by new physical principles at higher energies in the early universe. Additionally, there are many unresolved issues related to these problems, but\n",
      "there might be reasons to be optimistic in the near future.\n",
      "\n",
      "\n",
      "Q9: What future technologies are anticipated to advance our exploration of cosmological phenomena?\n",
      " response:  Based on the context provided, the\n",
      "article discusses the potential application of Augmented Reality (AR) technology in advancing our exploration of cosmological phenomena. It mentions\n",
      "that AR is becoming a mature technology that can be easily used and implemented by non-experts, leading to an increase in the number of AR tools and\n",
      "apps. Additionally, the article suggests that AR could potentially offer new ways of communicating scientific results and data through social media\n",
      "and publications. However, the article also acknowledges some limitations of AR in astrophysical research, including complexity, accessibility, and\n",
      "long-term stability. Therefore, while the article does not specifically mention other future technologies, it emphasizes the importance and potential\n",
      "of AR in advancing our understanding of cosmological phenomena.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tUjRknDAav7"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'AstroPhysicsQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV1OthB8Aay2"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'AstroPhysicsQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POtSGTQFCBk_"
   },
   "source": [
    "### Attention Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHRdqhM-Aa2L",
    "outputId": "803516d3-524e-4c51-87a8-64b48324028e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer model introduces the use of an attention mechanism instead of\n",
      "recurrence or convolutions for drawing global dependencies between input and output.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer model enhances training efficiency by utilizing attention\n",
      "mechanisms exclusively, eliminating the need for recurrence and convolutions. This simplified architecture allows for greater parallelizability and\n",
      "reduces the amount of time required to train compared to traditional RNN and Convolutional Neural Network (CNN) models.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer's encoder consists of a stack of N identical layers, each\n",
      "containing a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. Both sub-layers have residual\n",
      "connections and layer normalization. All sub-layers and embedding layers produce outputs of dimension dmodel = 512. The encoder uses self-attention\n",
      "layers, allowing each position to attend to all positions in the previous layer within the encoder.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-head attention in the Transformer allows the model to jointly\n",
      "attend to information from different representation subspaces at different positions. It reduces the effect of averaging attention-weighted positions,\n",
      "which is a challenge in models like ConvS2S and ByteNet due to their reliance on positional encoding. Multi-head attention consists of several\n",
      "attention layers running in parallel, each focusing on different subspaces.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer handles sequence order through attention mechanisms, allowing\n",
      "modeling of dependencies without regard to their distance in the input or output sequences, rather than using recurrence like most other competitive\n",
      "models.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Self-attention in the Transformer reduces the computational complexity\n",
      "compared to models like ConvS2S and ByteNet when attending to positions with large distances. It allows each position in the encoder and decoder to\n",
      "attend to all positions within itself, reducing the need for sequential processing. Additionally, multi-head attention is used to counteract the\n",
      "reduced effective resolution caused by averaging attention-weighted positions. Self-attention has been successful in various tasks such as reading\n",
      "comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations. The Transformer is the first\n",
      "transduction model to rely entirely on self-attention for computing input and output representations without using sequence-aligned RNNs or\n",
      "convolutions.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention in Transformers is a mechanism that maps a query and a set of key-value pairs to an\n",
      "output, where the query, keys, values, and output are all vectors. It allows every position in one sequence (decoder or encoder) to attend to all\n",
      "positions in another sequence (input or previous layer). Multi-head attention enables the model to jointly attend to information from different\n",
      "representation subspaces at different positions.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Residual dropout is applied to the output of each sub-layer before it is\n",
      "added to the sub-layer input and normalized. Additionally, dropout is applied to the sums of the embeddings and the positional encodings in both the\n",
      "encoder and decoder stacks. For the base model, a dropout rate of 0.1 is used.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer model improved machine translation accuracy\n",
      "through the use of attention mechanisms in place of recurrence and convolutions. It achieved superior quality on two machine translation tasks\n",
      "compared to previous models, while being more parallelizable and requiring less time to train. Specifically, it established new state-of-the-art BLEU\n",
      "scores on the WMT 2014 English-to-German and English-to-French translation tasks. Additionally, it generalized well to other tasks such as English\n",
      "constituency parsing.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      " Yes, the Transformer model has been shown to generalize well to\n",
      "other tasks beyond machine translation. It has been successfully applied to English constituency parsing with both large and limited training data.\n",
      "Additionally, there are plans to extend the Transformer to problems involving input and output modalities other than text and to investigate local,\n",
      "restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n",
      "/content/papers/Attention.pdf\n"
     ]
    }
   ],
   "source": [
    "# Attention Questions\n",
    "\n",
    "questions_file_path = '/content/questions/AttQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWlOMKZNAa48",
    "outputId": "232b6221-62f9-4fb4-a201-165b32a89337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What innovation does the Transformer model introduce?\n",
      " response:  The Transformer model introduces the use of an attention mechanism instead of\n",
      "recurrence or convolutions for drawing global dependencies between input and output.\n",
      "\n",
      "\n",
      "Q1: How does the Transformer model enhance training efficiency?\n",
      " response:  The Transformer model enhances training efficiency by utilizing attention\n",
      "mechanisms exclusively, eliminating the need for recurrence and convolutions. This simplified architecture allows for greater parallelizability and\n",
      "reduces the amount of time required to train compared to traditional RNN and Convolutional Neural Network (CNN) models.\n",
      "\n",
      "\n",
      "Q2: What are key components of the Transformer’s encoder?\n",
      " response:  The Transformer's encoder consists of a stack of N identical layers, each\n",
      "containing a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. Both sub-layers have residual\n",
      "connections and layer normalization. All sub-layers and embedding layers produce outputs of dimension dmodel = 512. The encoder uses self-attention\n",
      "layers, allowing each position to attend to all positions in the previous layer within the encoder.\n",
      "\n",
      "\n",
      "Q3: What is the function of multi-head attention in the Transformer?\n",
      " response:  Multi-head attention in the Transformer allows the model to jointly\n",
      "attend to information from different representation subspaces at different positions. It reduces the effect of averaging attention-weighted positions,\n",
      "which is a challenge in models like ConvS2S and ByteNet due to their reliance on positional encoding. Multi-head attention consists of several\n",
      "attention layers running in parallel, each focusing on different subspaces.\n",
      "\n",
      "\n",
      "Q4: How does the Transformer handle sequence order?\n",
      " response:  The Transformer handles sequence order through attention mechanisms, allowing\n",
      "modeling of dependencies without regard to their distance in the input or output sequences, rather than using recurrence like most other competitive\n",
      "models.\n",
      "\n",
      "\n",
      "Q5: What are the benefits of self-attention in the Transformer?\n",
      " response:  Self-attention in the Transformer reduces the computational complexity\n",
      "compared to models like ConvS2S and ByteNet when attending to positions with large distances. It allows each position in the encoder and decoder to\n",
      "attend to all positions within itself, reducing the need for sequential processing. Additionally, multi-head attention is used to counteract the\n",
      "reduced effective resolution caused by averaging attention-weighted positions. Self-attention has been successful in various tasks such as reading\n",
      "comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations. The Transformer is the first\n",
      "transduction model to rely entirely on self-attention for computing input and output representations without using sequence-aligned RNNs or\n",
      "convolutions.\n",
      "\n",
      "\n",
      "Q6: What is attention in Transformers?\n",
      " response:  Attention in Transformers is a mechanism that maps a query and a set of key-value pairs to an\n",
      "output, where the query, keys, values, and output are all vectors. It allows every position in one sequence (decoder or encoder) to attend to all\n",
      "positions in another sequence (input or previous layer). Multi-head attention enables the model to jointly attend to information from different\n",
      "representation subspaces at different positions.\n",
      "\n",
      "\n",
      "Q7: What regularization techniques are used in the Transformer?\n",
      " response:  Residual dropout is applied to the output of each sub-layer before it is\n",
      "added to the sub-layer input and normalized. Additionally, dropout is applied to the sums of the embeddings and the positional encodings in both the\n",
      "encoder and decoder stacks. For the base model, a dropout rate of 0.1 is used.\n",
      "\n",
      "\n",
      "Q8: How did the Transformer model improve machine translation accuracy?\n",
      " response:  The Transformer model improved machine translation accuracy\n",
      "through the use of attention mechanisms in place of recurrence and convolutions. It achieved superior quality on two machine translation tasks\n",
      "compared to previous models, while being more parallelizable and requiring less time to train. Specifically, it established new state-of-the-art BLEU\n",
      "scores on the WMT 2014 English-to-German and English-to-French translation tasks. Additionally, it generalized well to other tasks such as English\n",
      "constituency parsing.\n",
      "\n",
      "\n",
      "Q9: Can the Transformer model be applied beyond machine translation?\n",
      " response:  Yes, the Transformer model has been shown to generalize well to\n",
      "other tasks beyond machine translation. It has been successfully applied to English constituency parsing with both large and limited training data.\n",
      "Additionally, there are plans to extend the Transformer to problems involving input and output modalities other than text and to investigate local,\n",
      "restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnmw05WDCRbt"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'AttQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9Zjmnb0CTyb"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'AttQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5BK_Q6FCZJm"
   },
   "source": [
    "###Coherent Spin Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePOCH8wECb92",
    "outputId": "a755f462-f338-4b30-f987-a9a3046ff724"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The objective of using Heisenberg exchange in quantum\n",
      "computing is to manipulate and transfer quantum states between particles with high precision and minimal loss of coherence. However, the concept of\n",
      "coherent spin-state transfer via Heisenberg exchange is frequently misunderstood and misrepresented within the framework of classical mechanics. It is\n",
      "essential to clarify the distinct nature of quantum phenomena and rectify fundamental misunderstandings to accurately understand quantum mechanics in\n",
      "modern technological applications.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Heisenberg exchange\n",
      "interaction during quantum spin-state transfer is influenced by the exchange coupling constant, denoted by 𝐽. It is important to note that this\n",
      "constant is a quantum mechanical property and should not be compared to a classical spring constant. Other factors include the spin vectors of the\n",
      "interacting particles, 𝑆𝑖 and 𝑆𝑗. However, the misconception lies in the inclusion of non-existent gravitational interactions between nearest neighbor\n",
      "pairs in the sum, denoted by ⟨𝑖,𝑗⟩. These interactions do not exist in the Heisenberg exchange interaction.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Coherent spin-state transfer contributes to\n",
      "advancements in quantum technology by enabling precise manipulation and management of quantum states with minimal loss of coherence. It is a critical\n",
      "aspect of quantum computing and information processing, allowing for the development of innovative technologies such as superfast quantum computers,\n",
      "unbreakable quantum cryptography, and ultrasensitive quantum sensors.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A quantum system is typically prepared for spin-state transfer using\n",
      "various methods depending on the specific system and application. In the context of the given paper, the authors discuss the use of an entanglement\n",
      "swapping protocol to prepare a teleporter for qubit teleportation between non-neighboring network nodes. This involves establishing entanglement\n",
      "between two neighboring nodes and then using that entanglement to transfer the state of a third node. However, the exact method for preparing a\n",
      "quantum system for spin-state transfer can vary widely and may involve different techniques such as magnetic fields, resonant pulses, or other forms\n",
      "of control.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Heisenberg exchange interaction is a\n",
      "quantum mechanical effect that governs the interaction between spins in a system. It plays an essential role in various quantum phenomena, including\n",
      "quantum computing and information processing. However, it is important to note that the Heisenberg exchange interaction itself does not directly\n",
      "facilitate quantum computing operations. Instead, it underlies many quantum algorithms and processes, such as quantum error correction, quantum\n",
      "annealing, and quantum simulation, by enabling the manipulation and control of quantum states. These quantum states represent the qubits in a quantum\n",
      "computer, allowing for complex computational tasks to be performed with high precision and minimal loss of coherence.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Electric fields can influence quantum state dynamics\n",
      "in spin-state transfers through their interaction with the magnetic moments of the spins involved. However, the text discusses misconceptions about\n",
      "maintaining thermal dynamics and comparing quantum spins to classical objects during spin-state transfers, which are not directly related to the\n",
      "influence of electric fields.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text discusses the importance of correctly\n",
      "understanding quantum mechanics concepts, specifically regarding coherent spin-state transfer via the Heisenberg exchange interaction, to avoid\n",
      "misconceptions and improve the accuracy of scientific discourse in modern technological applications, including quantum computing. It mentions the\n",
      "misconception of comparing quantum state dynamics to mechanical and thermal interactions, which are irrelevant and misleading in the quantum realm.\n",
      "However, there is no specific mention or discussion about quantum scalability related to coherent spin-state transfer in the given text.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Heisenberg exchange interaction plays a\n",
      "significant role in quantum computing applications, specifically in managing and manipulating quantum states with high precision and minimal loss of\n",
      "coherence. It is a quantum mechanical effect that dictates the interaction between spins in a system, but is often misunderstood due to its\n",
      "misrepresentation as a classical phenomenon. Incorrect assumptions about the nature of quantum phenomena and the use of classical terms like\n",
      "temperature and mechanical forces can lead to fundamental misunderstandings that complicate the accurate understanding of quantum mechanics in modern\n",
      "technological applications.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The metaphor commonly used to explain the initialization of\n",
      "quantum systems is confusing it with setting up a classical mechanical system, such as loading a spring. However, this analogy is misleading as\n",
      "quantum initialization involves putting a quantum system into a specific quantum state, while classical mechanical initialization refers to preparing\n",
      "a physical system for motion.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/quantum.pdf\n",
      " Future research in quantum mechanics is expected to focus on\n",
      "developing educational tools and resources to distinguish between classical and quantum physical theories, preventing the perpetuation of\n",
      "misconceptions in scientific and technological advancements. Additionally, new fields of knowledge such as quantum information theory, quantum\n",
      "thermodynamics, and the development of novel mathematical and computational tools will continue to emerge. Quantum science is also achieving concrete\n",
      "impacts through the development of technologies like superfast quantum computers, unbreakable quantum cryptography, and ultrasensitive quantum\n",
      "sensors. The exploration of the interplay of quantum mechanics with black-hole physics, thermodynamics, and the emergence of the classical world via\n",
      "quantum principles is also ongoing. Recent progress on topological quantum computing and prospects of a quantum internet, as well as experimental\n",
      "advances in satellite-based quantum communication and verification of basic laws of quantum theory, are also being investigated.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    }
   ],
   "source": [
    "# Coherent Spin Questions\n",
    "\n",
    "questions_file_path = '/content/questions/CoSpinQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHeW0aTkC7Qe",
    "outputId": "b0366f3d-196f-42ae-dd33-691e6bfd9ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What is the objective of using Heisenberg exchange in quantum computing?\n",
      " response:  The objective of using Heisenberg exchange in quantum\n",
      "computing is to manipulate and transfer quantum states between particles with high precision and minimal loss of coherence. However, the concept of\n",
      "coherent spin-state transfer via Heisenberg exchange is frequently misunderstood and misrepresented within the framework of classical mechanics. It is\n",
      "essential to clarify the distinct nature of quantum phenomena and rectify fundamental misunderstandings to accurately understand quantum mechanics in\n",
      "modern technological applications.\n",
      "\n",
      "\n",
      "Q1: What external factors influence the Heisenberg exchange interaction during quantum spin-state transfer?\n",
      " response:  The Heisenberg exchange\n",
      "interaction during quantum spin-state transfer is influenced by the exchange coupling constant, denoted by 𝐽. It is important to note that this\n",
      "constant is a quantum mechanical property and should not be compared to a classical spring constant. Other factors include the spin vectors of the\n",
      "interacting particles, 𝑆𝑖 and 𝑆𝑗. However, the misconception lies in the inclusion of non-existent gravitational interactions between nearest neighbor\n",
      "pairs in the sum, denoted by ⟨𝑖,𝑗⟩. These interactions do not exist in the Heisenberg exchange interaction.\n",
      "\n",
      "\n",
      "Q2: How does coherent spin-state transfer contribute to advancements in quantum technology?\n",
      " response:  Coherent spin-state transfer contributes to\n",
      "advancements in quantum technology by enabling precise manipulation and management of quantum states with minimal loss of coherence. It is a critical\n",
      "aspect of quantum computing and information processing, allowing for the development of innovative technologies such as superfast quantum computers,\n",
      "unbreakable quantum cryptography, and ultrasensitive quantum sensors.\n",
      "\n",
      "\n",
      "Q3: How is a quantum system prepared for spin-state transfer?\n",
      " response:  A quantum system is typically prepared for spin-state transfer using\n",
      "various methods depending on the specific system and application. In the context of the given paper, the authors discuss the use of an entanglement\n",
      "swapping protocol to prepare a teleporter for qubit teleportation between non-neighboring network nodes. This involves establishing entanglement\n",
      "between two neighboring nodes and then using that entanglement to transfer the state of a third node. However, the exact method for preparing a\n",
      "quantum system for spin-state transfer can vary widely and may involve different techniques such as magnetic fields, resonant pulses, or other forms\n",
      "of control.\n",
      "\n",
      "\n",
      "Q4: How does the Heisenberg exchange interaction facilitate quantum computing operations?\n",
      " response:  The Heisenberg exchange interaction is a\n",
      "quantum mechanical effect that governs the interaction between spins in a system. It plays an essential role in various quantum phenomena, including\n",
      "quantum computing and information processing. However, it is important to note that the Heisenberg exchange interaction itself does not directly\n",
      "facilitate quantum computing operations. Instead, it underlies many quantum algorithms and processes, such as quantum error correction, quantum\n",
      "annealing, and quantum simulation, by enabling the manipulation and control of quantum states. These quantum states represent the qubits in a quantum\n",
      "computer, allowing for complex computational tasks to be performed with high precision and minimal loss of coherence.\n",
      "\n",
      "\n",
      "Q5: How do electric fields influence quantum state dynamics in spin-state transfers?\n",
      " response:  Electric fields can influence quantum state dynamics\n",
      "in spin-state transfers through their interaction with the magnetic moments of the spins involved. However, the text discusses misconceptions about\n",
      "maintaining thermal dynamics and comparing quantum spins to classical objects during spin-state transfers, which are not directly related to the\n",
      "influence of electric fields.\n",
      "\n",
      "\n",
      "Q6: How is quantum scalability addressed in discussions of coherent spin-state transfer?\n",
      " response:  The text discusses the importance of correctly\n",
      "understanding quantum mechanics concepts, specifically regarding coherent spin-state transfer via the Heisenberg exchange interaction, to avoid\n",
      "misconceptions and improve the accuracy of scientific discourse in modern technological applications, including quantum computing. It mentions the\n",
      "misconception of comparing quantum state dynamics to mechanical and thermal interactions, which are irrelevant and misleading in the quantum realm.\n",
      "However, there is no specific mention or discussion about quantum scalability related to coherent spin-state transfer in the given text.\n",
      "\n",
      "\n",
      "Q7: What role does the Heisenberg exchange play in quantum computing applications?\n",
      " response:  The Heisenberg exchange interaction plays a\n",
      "significant role in quantum computing applications, specifically in managing and manipulating quantum states with high precision and minimal loss of\n",
      "coherence. It is a quantum mechanical effect that dictates the interaction between spins in a system, but is often misunderstood due to its\n",
      "misrepresentation as a classical phenomenon. Incorrect assumptions about the nature of quantum phenomena and the use of classical terms like\n",
      "temperature and mechanical forces can lead to fundamental misunderstandings that complicate the accurate understanding of quantum mechanics in modern\n",
      "technological applications.\n",
      "\n",
      "\n",
      "Q8: What metaphors are used to explain the initialization of quantum systems?\n",
      " response:  The metaphor commonly used to explain the initialization of\n",
      "quantum systems is confusing it with setting up a classical mechanical system, such as loading a spring. However, this analogy is misleading as\n",
      "quantum initialization involves putting a quantum system into a specific quantum state, while classical mechanical initialization refers to preparing\n",
      "a physical system for motion.\n",
      "\n",
      "\n",
      "Q9: What future developments are expected in quantum mechanics research?\n",
      " response:  Future research in quantum mechanics is expected to focus on\n",
      "developing educational tools and resources to distinguish between classical and quantum physical theories, preventing the perpetuation of\n",
      "misconceptions in scientific and technological advancements. Additionally, new fields of knowledge such as quantum information theory, quantum\n",
      "thermodynamics, and the development of novel mathematical and computational tools will continue to emerge. Quantum science is also achieving concrete\n",
      "impacts through the development of technologies like superfast quantum computers, unbreakable quantum cryptography, and ultrasensitive quantum\n",
      "sensors. The exploration of the interplay of quantum mechanics with black-hole physics, thermodynamics, and the emergence of the classical world via\n",
      "quantum principles is also ongoing. Recent progress on topological quantum computing and prospects of a quantum internet, as well as experimental\n",
      "advances in satellite-based quantum communication and verification of basic laws of quantum theory, are also being investigated.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFoJ5VRiC8Bh"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'CoSpinQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23pmj4wMDDBK"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'CoSpinQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFX0IazTDT2U"
   },
   "source": [
    "###Mamba Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zq9gIgZ7DVVm",
    "outputId": "7722c389-3295-4742-f4c4-624e847d6813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mamba achieves computational efficiency\n",
      "by operating effectively under generic GPU configurations, assuming that standard GPU capabilities are sufficient to manage its computational demands\n",
      "efficiently.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mamba introduces selective state space models, which\n",
      "dynamically adjust model parameters based on the output, optimizing real-time processing capabilities and adjusting computational focus dynamically.\n",
      "It also extends the application of structured state space models (SSMs) to sequence data, providing a more scalable approach to sequence modeling.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main\n",
      "difference between selective state space models in Mamba and traditional attention mechanisms lies in how they handle sequence data and adjust model\n",
      "parameters. Traditional attention mechanisms rely solely on input-dependent parameter adjustments, while selective state space models in Mamba use a\n",
      "reverse-feedback mechanism to dynamically adjust model parameters based on the output. This allows for optimizing real-time processing capabilities\n",
      "and adjusting computational focus dynamically.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The limitations of Mamba's approach to sequence modeling include\n",
      "its potential restricted modification and use within the broader AI community due to its projected release strategy, and its currently unexplored\n",
      "performance on long sequences and complex datasets compared to well-established models like Transformers.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mamba simplifies the integration of RNN-like\n",
      "and CNN-like layers by merging them into one architecture.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The potential challenge mentioned in the\n",
      "context text is the restrictive release strategy of Mamba, which may limit its modification and use within the broader AI community. This could impact\n",
      "its adoption and further development across diverse applications beyond video processing and short text snippets.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n",
      "/content/papers/Augmented Reality in astrophysics.pdf\n",
      "/content/papers/Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mamba's preliminary evaluations\n",
      "indicate that its effectiveness on longer sequences and more complex datasets needs thorough benchmarking against established models like\n",
      "Transformers. Additionally, the open-sourcing strategy and potential limitations on modifications may impact its adoption and further development\n",
      "across diverse applications.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dynamic parameter adjustment is a key\n",
      "feature of Mamba's selective state space models, allowing for optimized real-time processing capabilities and dynamic computation focus, shifting from\n",
      "input-dependent parameter adjustments seen in traditional models.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mamba addresses the inefficiencies\n",
      "of Transformers in moderate to long sequence processing by leveraging structured state space models (SSMs) and integrating RNN-like and CNN-like\n",
      "layers while reintroducing MLP blocks. It operates effectively under standard GPU configurations and reduces dependency on traditional attention\n",
      "mechanisms.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      " The design of Mamba, which\n",
      "utilizes structured state space models (SSMs) and merges RNN-like and CNN-like layers, allows it to process various data types more efficiently\n",
      "compared to traditional methods. However, its effectiveness on long sequences and complex datasets, especially when compared to established models\n",
      "like Transformers, has not been extensively evaluated yet. Additionally, the restricted release strategy for Mamba may limit its modification and use\n",
      "within the broader AI community, potentially impacting its adoption and development across diverse applications.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n",
      "/content/papers/Mamba.pdf\n"
     ]
    }
   ],
   "source": [
    "# Mamba Questions\n",
    "\n",
    "questions_file_path = '/content/questions/MambaQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtCHuDu1De8w",
    "outputId": "379ce177-ae1b-46c6-c499-1eb55b04799f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: How does Mamba achieve computational efficiency without specialized hardware optimizations?\n",
      " response:  Mamba achieves computational efficiency\n",
      "by operating effectively under generic GPU configurations, assuming that standard GPU capabilities are sufficient to manage its computational demands\n",
      "efficiently.\n",
      "\n",
      "\n",
      "Q1: What innovations does Mamba introduce to manage long sequence data processing?\n",
      " response:  Mamba introduces selective state space models, which\n",
      "dynamically adjust model parameters based on the output, optimizing real-time processing capabilities and adjusting computational focus dynamically.\n",
      "It also extends the application of structured state space models (SSMs) to sequence data, providing a more scalable approach to sequence modeling.\n",
      "\n",
      "\n",
      "Q2: How does the selective state space model in Mamba differ from traditional attention mechanisms in handling sequence data?\n",
      " response:  The main\n",
      "difference between selective state space models in Mamba and traditional attention mechanisms lies in how they handle sequence data and adjust model\n",
      "parameters. Traditional attention mechanisms rely solely on input-dependent parameter adjustments, while selective state space models in Mamba use a\n",
      "reverse-feedback mechanism to dynamically adjust model parameters based on the output. This allows for optimizing real-time processing capabilities\n",
      "and adjusting computational focus dynamically.\n",
      "\n",
      "\n",
      "Q3: What are the limitations of Mamba’s approach to sequence modeling?\n",
      " response:  The limitations of Mamba's approach to sequence modeling include\n",
      "its potential restricted modification and use within the broader AI community due to its projected release strategy, and its currently unexplored\n",
      "performance on long sequences and complex datasets compared to well-established models like Transformers.\n",
      "\n",
      "\n",
      "Q4: How does Mamba’s architecture simplify the integration of RNN-like and CNN-like layers?\n",
      " response:  Mamba simplifies the integration of RNN-like\n",
      "and CNN-like layers by merging them into one architecture.\n",
      "\n",
      "\n",
      "Q5: What potential challenges might restrict the open-sourcing and wider adoption of Mamba?\n",
      " response:  The potential challenge mentioned in the\n",
      "context text is the restrictive release strategy of Mamba, which may limit its modification and use within the broader AI community. This could impact\n",
      "its adoption and further development across diverse applications beyond video processing and short text snippets.\n",
      "\n",
      "\n",
      "Q6: In what ways does Mamba's performance evaluation suggest areas for future research and development?\n",
      " response:  Mamba's preliminary evaluations\n",
      "indicate that its effectiveness on longer sequences and more complex datasets needs thorough benchmarking against established models like\n",
      "Transformers. Additionally, the open-sourcing strategy and potential limitations on modifications may impact its adoption and further development\n",
      "across diverse applications.\n",
      "\n",
      "\n",
      "Q7: What role does dynamic parameter adjustment play in Mamba’s selective state space models?\n",
      " response:  Dynamic parameter adjustment is a key\n",
      "feature of Mamba's selective state space models, allowing for optimized real-time processing capabilities and dynamic computation focus, shifting from\n",
      "input-dependent parameter adjustments seen in traditional models.\n",
      "\n",
      "\n",
      "Q8: How does Mamba address the inefficiencies of Transformers in moderate to long sequence processing?\n",
      " response:  Mamba addresses the inefficiencies\n",
      "of Transformers in moderate to long sequence processing by leveraging structured state space models (SSMs) and integrating RNN-like and CNN-like\n",
      "layers while reintroducing MLP blocks. It operates effectively under standard GPU configurations and reduces dependency on traditional attention\n",
      "mechanisms.\n",
      "\n",
      "\n",
      "Q9: What implications does the design of Mamba have for its applicability across different data modalities?\n",
      " response:  The design of Mamba, which\n",
      "utilizes structured state space models (SSMs) and merges RNN-like and CNN-like layers, allows it to process various data types more efficiently\n",
      "compared to traditional methods. However, its effectiveness on long sequences and complex datasets, especially when compared to established models\n",
      "like Transformers, has not been extensively evaluated yet. Additionally, the restricted release strategy for Mamba may limit its modification and use\n",
      "within the broader AI community, potentially impacting its adoption and development across diverse applications.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FM3u9a1DgXN"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'MambaQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_vuJeSUDhnN"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'MambaQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOBp2sMoDmC5"
   },
   "source": [
    "###Parametric Magnon Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIE3Or1yDqez",
    "outputId": "10bc79f1-e6f0-47f7-f6ca-ad5aedd87ac1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The typical materials used to construct\n",
      "quantum transducers in hybrid quantum systems are yttrium iron garnet (YIG) and nitrogen-vacancy (NV) defects in diamond. However, the approach\n",
      "presented in the given work uses wafer-compatible materials to engineer a hybrid transducer that exploits magnon nonlinearities in a magnetic\n",
      "microdisc to address quantum spin defects in silicon carbide.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Magnon nonlinearities enhance quantum transduction by allowing selective\n",
      "tuning of the spin-magnet coupling \"on\" and \"off,\" providing protection against resonant magnon noise-induced decoherence, and enabling tunable\n",
      "transduction between distinct physical components. This contrasts with the constant dipolar coupling between spin centers and linear magnons in\n",
      "traditional systems.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One technique used to minimize the\n",
      "microwave footprint in quantum computing transducers is by using highly-confined magnon stray fields to drive the spin qubits at room temperature,\n",
      "rather than relying on external microwave driving fields from antennas which typically extend over larger areas. Another approach is to harness\n",
      "parametric magnon effects to downconvert the microwave driving frequency and address off-resonant ensembles of spin qubits, further reducing the\n",
      "required microwave power. Additionally, controlling the spin-magnon coupling \"on\" and \"off\" can protect the spin centers against resonant magnon\n",
      "noise-induced decoherence, thereby improving overall system performance.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Silicon carbide is favored in some quantum computing\n",
      "applications due to its compatibility with industry-standard fabrication protocols and ease of integration into heterogeneous quantum architectures.\n",
      "It also exhibits strong spin-magnon coupling, which can lead to protected qubits and reduced spillover cross-talk when scaling up quantum chips.\n",
      "(References: 53-55)\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Experimental validation of quantum\n",
      "transducers involves measuring the efficiency and accuracy of the transduction process between different physical systems, such as magnons and spin\n",
      "qubits. This typically includes characterization of the input and output signals, measurement of the coupling strength between the systems, and\n",
      "assessment of any sources of noise or decoherence. Techniques like spectroscopy, magnetometry, and electrical measurements may be used depending on\n",
      "the specific design of the transducer. Additionally, calibration and comparison with theoretical models or simulations are essential for understanding\n",
      "the performance and optimizing the operation of the device.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Frequency tuning is important in quantum\n",
      "transducers using magnon interactions because it enables selective tuning of the spin-magon coupling \"on\" and \"off.\" This control over the coupling\n",
      "strength can protect spin qubits from resonant magnon noise and enable unique transduction behaviors different from those observed in hybrid systems\n",
      "with linearly-excited magnons.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Parametric magnonics introduces a new approach\n",
      "in quantum transducers by using nonlinear magnonics to engineer a hybrid transducer that exploits magnon nonlinearities in a magnetic microdisc to\n",
      "address quantum spin defects. This approach offers unique transduction behavior compared to traditional linear magnon transducers.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introducing nonlinear magnonics to quantum computing\n",
      "systems provides alternative perspectives for engineering quantum interfaces to spin qubits and motivates further research into uncovering the\n",
      "interesting phenomena lying at the intersection of nonlinear magnonics and quantum systems. It offers the potential for enhanced coupling strengths\n",
      "and cooperativities through the generation of squeezed magnon states.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Magnon nonlinearities in quantum systems could\n",
      "potentially lead to enhanced coupling strengths and cooperativities in hybrid quantum systems, exponentially enhancing their performance. These\n",
      "nonclassical states have the potential to bring systems into the strong coupling regime and reduce damping processes at cryogenic temperatures. Future\n",
      "applications could include quantum computing, where magnon nonlinearities could help expand the quantum engineer's toolbox and improve the\n",
      "functionality of existing quantum systems.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      " Recent research in quantum magnonics\n",
      "provides alternative perspectives for engineering quantum interfaces to spin qubits by exploring nonlinear magnonic systems. These systems offer\n",
      "unique functionalities due to their wide range of magnon interactions and intrinsic nonlinear phenomena, making them especially promising for quantum\n",
      "computing applications. However, most work in quantum magnonics has focused on linear magnon dynamics, while nonlinear magnon-based transducers remain\n",
      "relatively unexplored. Engineered hybrid transducers that exploit magnon nonlinearities in magnetic microdiscs can address quantum spin defects in\n",
      "silicon carbide, leading to unique transduction behaviors.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/Parametric magnon.pdf\n"
     ]
    }
   ],
   "source": [
    "# Parametric Magnon Questions\n",
    "\n",
    "questions_file_path = '/content/questions/ParametricMagnonQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnUQW8FOD_xi",
    "outputId": "295d16e9-1aed-490e-aea5-749b5b22f36c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What materials are typically used to construct quantum transducers in hybrid quantum systems?\n",
      " response:  The typical materials used to construct\n",
      "quantum transducers in hybrid quantum systems are yttrium iron garnet (YIG) and nitrogen-vacancy (NV) defects in diamond. However, the approach\n",
      "presented in the given work uses wafer-compatible materials to engineer a hybrid transducer that exploits magnon nonlinearities in a magnetic\n",
      "microdisc to address quantum spin defects in silicon carbide.\n",
      "\n",
      "\n",
      "Q1: How do magnon nonlinearities enhance quantum transduction?\n",
      " response:  Magnon nonlinearities enhance quantum transduction by allowing selective\n",
      "tuning of the spin-magnet coupling \"on\" and \"off,\" providing protection against resonant magnon noise-induced decoherence, and enabling tunable\n",
      "transduction between distinct physical components. This contrasts with the constant dipolar coupling between spin centers and linear magnons in\n",
      "traditional systems.\n",
      "\n",
      "\n",
      "Q2: What techniques are used to minimize the microwave footprint in quantum computing transducers?\n",
      " response:  One technique used to minimize the\n",
      "microwave footprint in quantum computing transducers is by using highly-confined magnon stray fields to drive the spin qubits at room temperature,\n",
      "rather than relying on external microwave driving fields from antennas which typically extend over larger areas. Another approach is to harness\n",
      "parametric magnon effects to downconvert the microwave driving frequency and address off-resonant ensembles of spin qubits, further reducing the\n",
      "required microwave power. Additionally, controlling the spin-magnon coupling \"on\" and \"off\" can protect the spin centers against resonant magnon\n",
      "noise-induced decoherence, thereby improving overall system performance.\n",
      "\n",
      "\n",
      "Q3: Why is silicon carbide favored in some quantum computing applications?\n",
      " response:  Silicon carbide is favored in some quantum computing\n",
      "applications due to its compatibility with industry-standard fabrication protocols and ease of integration into heterogeneous quantum architectures.\n",
      "It also exhibits strong spin-magnon coupling, which can lead to protected qubits and reduced spillover cross-talk when scaling up quantum chips.\n",
      "(References: 53-55)\n",
      "\n",
      "\n",
      "Q4: What experimental methods are crucial for validating the functionality of quantum transducers?\n",
      " response:  Experimental validation of quantum\n",
      "transducers involves measuring the efficiency and accuracy of the transduction process between different physical systems, such as magnons and spin\n",
      "qubits. This typically includes characterization of the input and output signals, measurement of the coupling strength between the systems, and\n",
      "assessment of any sources of noise or decoherence. Techniques like spectroscopy, magnetometry, and electrical measurements may be used depending on\n",
      "the specific design of the transducer. Additionally, calibration and comparison with theoretical models or simulations are essential for understanding\n",
      "the performance and optimizing the operation of the device.\n",
      "\n",
      "\n",
      "Q5: What is the importance of frequency tuning in quantum transducers using magnon interactions?\n",
      " response:  Frequency tuning is important in quantum\n",
      "transducers using magnon interactions because it enables selective tuning of the spin-magon coupling \"on\" and \"off.\" This control over the coupling\n",
      "strength can protect spin qubits from resonant magnon noise and enable unique transduction behaviors different from those observed in hybrid systems\n",
      "with linearly-excited magnons.\n",
      "\n",
      "\n",
      "Q6: What innovative approach does parametric magnonics introduce in quantum transducers?\n",
      " response:  Parametric magnonics introduces a new approach\n",
      "in quantum transducers by using nonlinear magnonics to engineer a hybrid transducer that exploits magnon nonlinearities in a magnetic microdisc to\n",
      "address quantum spin defects. This approach offers unique transduction behavior compared to traditional linear magnon transducers.\n",
      "\n",
      "\n",
      "Q7: How does introducing nonlinear magnonics impact quantum computing systems?\n",
      " response:  Introducing nonlinear magnonics to quantum computing\n",
      "systems provides alternative perspectives for engineering quantum interfaces to spin qubits and motivates further research into uncovering the\n",
      "interesting phenomena lying at the intersection of nonlinear magnonics and quantum systems. It offers the potential for enhanced coupling strengths\n",
      "and cooperativities through the generation of squeezed magnon states.\n",
      "\n",
      "\n",
      "Q8: What future applications could benefit from magnon nonlinearities in quantum systems?\n",
      " response:  Magnon nonlinearities in quantum systems could\n",
      "potentially lead to enhanced coupling strengths and cooperativities in hybrid quantum systems, exponentially enhancing their performance. These\n",
      "nonclassical states have the potential to bring systems into the strong coupling regime and reduce damping processes at cryogenic temperatures. Future\n",
      "applications could include quantum computing, where magnon nonlinearities could help expand the quantum engineer's toolbox and improve the\n",
      "functionality of existing quantum systems.\n",
      "\n",
      "\n",
      "Q9: How does recent research in quantum magnonics influence the design of quantum interfaces?\n",
      " response:  Recent research in quantum magnonics\n",
      "provides alternative perspectives for engineering quantum interfaces to spin qubits by exploring nonlinear magnonic systems. These systems offer\n",
      "unique functionalities due to their wide range of magnon interactions and intrinsic nonlinear phenomena, making them especially promising for quantum\n",
      "computing applications. However, most work in quantum magnonics has focused on linear magnon dynamics, while nonlinear magnon-based transducers remain\n",
      "relatively unexplored. Engineered hybrid transducers that exploit magnon nonlinearities in magnetic microdiscs can address quantum spin defects in\n",
      "silicon carbide, leading to unique transduction behaviors.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNKuB30JEBWi"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'ParametricMagnonQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ns4l3PKbEC_G"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'ParametricMagnonQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQD1a_eoEKsM"
   },
   "source": [
    "###Quantum Mechanics Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6z4CERedEPiX",
    "outputId": "110f26ed-d5dc-4ffd-83c0-a85c4cd79cda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum mechanics is significant in modern technology because it\n",
      "has led to the creation of new fields of knowledge, such as quantum information theory and quantum thermodynamics, and the development of novel\n",
      "mathematical and computational tools applicable to various domains. Additionally, an improved understanding of the resource power of quantum phenomena\n",
      "has triggered a technological overhaul, resulting in the development of superfast quantum computers, unbreakable quantum cryptography, ultrasensitive\n",
      "quantum sensors, and other disruptive technologies. These innovations have the potential to rival the three major industrial revolutions of the last\n",
      "century.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum mechanics has led to the creation of new fields of study such as\n",
      "quantum information theory, quantum thermodynamics, and the development of novel mathematical and computational tools applicable to other domains\n",
      "including condensed matter physics, statistical mechanics, and cosmology.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum science research has led to the creation\n",
      "of new fields of knowledge such as quantum information theory, quantum thermodynamics, and the development of novel mathematical and computational\n",
      "tools applicable to other domains including condensed matter physics, statistical mechanics, and cosmology.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Advancements in quantum technologies have the\n",
      "potential to bring about significant societal impacts, including the development of superfast quantum computers, unbreakable quantum cryptography, and\n",
      "ultrasensitive quantum sensors. These innovations have the capacity to revolutionize various industries and trigger a technological overhaul, rivaling\n",
      "the three major industrial revolutions of the last century. Additionally, research in quantum science has led to the creation of new fields of\n",
      "knowledge, such as quantum information theory and quantum thermodynamics, and has achieved a concrete impact through an improved understanding of the\n",
      "resource power of quantum phenomena.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum phenomena serve as the foundation for several emerging\n",
      "technological innovations, leading to the creation of new fields of knowledge such as quantum information theory and quantum thermodynamics. These\n",
      "advancements have resulted in significant impacts on various domains, including condensed matter physics, statistical mechanics, and cosmology.\n",
      "Moreover, research in quantum science has driven the development of technologies like superfast quantum computers, unbreakable quantum cryptography,\n",
      "and ultrasensitive quantum sensors, which have captured the public's attention due to their potential societal benefits. Technology giants like\n",
      "Google, IBM, and Microsoft are actively investing in making quantum technology accessible to the masses, adding to the excitement surrounding these\n",
      "developments. Despite the remarkable progress made, there are still fundamental questions about the foundations of quantum theory that remain open,\n",
      "potentially paving the way for even more disruptive technologies.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Technology giants such as Google, IBM, and Microsoft\n",
      "are actively involved in making quantum technology a household commodity in the near future through their research and development efforts. They are\n",
      "striving to embrace the challenge of advancing quantum technology and have made significant strides in areas like quantum computing, quantum\n",
      "cryptography, and quantum sensing.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/Parametric magnon.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/Coherent Spin.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is considerable debate regarding the\n",
      "interpretation of the foundations of quantum mechanics, specifically concerning issues such as quantum measurement, quantum randomness, non-locality,\n",
      "particle indistinguishability, causality, and the nature of time. These topics continue to be explored at the forefront of quantum research.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Exploring quantum mechanics contributes to theoretical\n",
      "physics by shedding light on the physical meaning of fundamental quantum principles and pushing the boundaries of the quantum description of the\n",
      "world. It leads to advancements in various areas of quantum research, including quantum measurement, quantum randomness, non-locality, particle\n",
      "indistinguishability, causality, and the nature of time. These discoveries have significant implications for both basic science and technology.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/Coherent Spin.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decades of experimental verification and control of quantum\n",
      "systems have proven detractors of quantum mechanics wrong. Examples include experiments on superposition, wave-particle duality, uncertainty\n",
      "principle, entanglement, and non-locality. (References: 3-7)\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      " Anticipated future developments in quantum technology include the\n",
      "creation of new fields of knowledge such as quantum information theory and quantum thermodynamics, the development of novel mathematical and\n",
      "computational tools, and the achievement of concrete impacts through the improvement of understanding of quantum phenomena's resource power, leading\n",
      "to technological overhauls. Exciting prospects include superfast quantum computers, unbreakable quantum cryptography, ultrasensitive quantum sensors,\n",
      "exploration of the interplay of quantum mechanics with black-hole physics and thermodynamics, and the emergence of the classical world via quantum\n",
      "principles. Progress is being made on topological quantum computing and prospects of a quantum internet, as well as experimental advances in\n",
      "satellite-based quantum communication and verification of basic laws of quantum theory.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Coherent Spin.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n",
      "/content/papers/quantum.pdf\n"
     ]
    }
   ],
   "source": [
    "# Quantum Mechanics Questions\n",
    "\n",
    "questions_file_path = '/content/questions/QuantumMechanicsQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbmQY-poEX05",
    "outputId": "83a57a35-1a7a-4954-a6bc-bfda7d78ba21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What is the significance of quantum mechanics in modern technology?\n",
      " response:  Quantum mechanics is significant in modern technology because it\n",
      "has led to the creation of new fields of knowledge, such as quantum information theory and quantum thermodynamics, and the development of novel\n",
      "mathematical and computational tools applicable to various domains. Additionally, an improved understanding of the resource power of quantum phenomena\n",
      "has triggered a technological overhaul, resulting in the development of superfast quantum computers, unbreakable quantum cryptography, ultrasensitive\n",
      "quantum sensors, and other disruptive technologies. These innovations have the potential to rival the three major industrial revolutions of the last\n",
      "century.\n",
      "\n",
      "\n",
      "Q1: How has quantum mechanics influenced new fields of study?\n",
      " response:  Quantum mechanics has led to the creation of new fields of study such as\n",
      "quantum information theory, quantum thermodynamics, and the development of novel mathematical and computational tools applicable to other domains\n",
      "including condensed matter physics, statistical mechanics, and cosmology.\n",
      "\n",
      "\n",
      "Q2: What advancements have quantum science research brought to other scientific domains?\n",
      " response:  Quantum science research has led to the creation\n",
      "of new fields of knowledge such as quantum information theory, quantum thermodynamics, and the development of novel mathematical and computational\n",
      "tools applicable to other domains including condensed matter physics, statistical mechanics, and cosmology.\n",
      "\n",
      "\n",
      "Q3: What are the potential societal impacts of advancements in quantum technologies?\n",
      " response:  Advancements in quantum technologies have the\n",
      "potential to bring about significant societal impacts, including the development of superfast quantum computers, unbreakable quantum cryptography, and\n",
      "ultrasensitive quantum sensors. These innovations have the capacity to revolutionize various industries and trigger a technological overhaul, rivaling\n",
      "the three major industrial revolutions of the last century. Additionally, research in quantum science has led to the creation of new fields of\n",
      "knowledge, such as quantum information theory and quantum thermodynamics, and has achieved a concrete impact through an improved understanding of the\n",
      "resource power of quantum phenomena.\n",
      "\n",
      "\n",
      "Q4: How do quantum phenomena underpin emerging technological innovations?\n",
      " response:  Quantum phenomena serve as the foundation for several emerging\n",
      "technological innovations, leading to the creation of new fields of knowledge such as quantum information theory and quantum thermodynamics. These\n",
      "advancements have resulted in significant impacts on various domains, including condensed matter physics, statistical mechanics, and cosmology.\n",
      "Moreover, research in quantum science has driven the development of technologies like superfast quantum computers, unbreakable quantum cryptography,\n",
      "and ultrasensitive quantum sensors, which have captured the public's attention due to their potential societal benefits. Technology giants like\n",
      "Google, IBM, and Microsoft are actively investing in making quantum technology accessible to the masses, adding to the excitement surrounding these\n",
      "developments. Despite the remarkable progress made, there are still fundamental questions about the foundations of quantum theory that remain open,\n",
      "potentially paving the way for even more disruptive technologies.\n",
      "\n",
      "\n",
      "Q5: What role do technology giants play in the advancement of quantum technologies?\n",
      " response:  Technology giants such as Google, IBM, and Microsoft\n",
      "are actively involved in making quantum technology a household commodity in the near future through their research and development efforts. They are\n",
      "striving to embrace the challenge of advancing quantum technology and have made significant strides in areas like quantum computing, quantum\n",
      "cryptography, and quantum sensing.\n",
      "\n",
      "\n",
      "Q6: What are the fundamental questions about quantum mechanics that remain open?\n",
      " response:  There is considerable debate regarding the\n",
      "interpretation of the foundations of quantum mechanics, specifically concerning issues such as quantum measurement, quantum randomness, non-locality,\n",
      "particle indistinguishability, causality, and the nature of time. These topics continue to be explored at the forefront of quantum research.\n",
      "\n",
      "\n",
      "Q7: How does exploring quantum mechanics contribute to theoretical physics?\n",
      " response:  Exploring quantum mechanics contributes to theoretical\n",
      "physics by shedding light on the physical meaning of fundamental quantum principles and pushing the boundaries of the quantum description of the\n",
      "world. It leads to advancements in various areas of quantum research, including quantum measurement, quantum randomness, non-locality, particle\n",
      "indistinguishability, causality, and the nature of time. These discoveries have significant implications for both basic science and technology.\n",
      "\n",
      "\n",
      "Q8: What experimental advances have been made in verifying quantum theory?\n",
      " response:  Decades of experimental verification and control of quantum\n",
      "systems have proven detractors of quantum mechanics wrong. Examples include experiments on superposition, wave-particle duality, uncertainty\n",
      "principle, entanglement, and non-locality. (References: 3-7)\n",
      "\n",
      "\n",
      "Q9: What future developments are anticipated in quantum technology?\n",
      " response:  Anticipated future developments in quantum technology include the\n",
      "creation of new fields of knowledge such as quantum information theory and quantum thermodynamics, the development of novel mathematical and\n",
      "computational tools, and the achievement of concrete impacts through the improvement of understanding of quantum phenomena's resource power, leading\n",
      "to technological overhauls. Exciting prospects include superfast quantum computers, unbreakable quantum cryptography, ultrasensitive quantum sensors,\n",
      "exploration of the interplay of quantum mechanics with black-hole physics and thermodynamics, and the emergence of the classical world via quantum\n",
      "principles. Progress is being made on topological quantum computing and prospects of a quantum internet, as well as experimental advances in\n",
      "satellite-based quantum communication and verification of basic laws of quantum theory.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyXwnHL1EZKl"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'QuantumMechanicsQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwcTgfULEcY4"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'Quantum MechanicsQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyyA_gUfEiDt"
   },
   "source": [
    "###Qubit Teleportation Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8y3tvjmEEmmW",
    "outputId": "9f7bddbd-b392-4f26-a944-aa27e3944885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main components used in quantum networks for\n",
      "teleportation include optically connected nodes, a teleporter in the form of a pre-shared entangled state, and a memory qubit for storing the\n",
      "teleported qubit state. Additionally, there are key innovations such as improved qubit readout procedures, active memory qubit protection during\n",
      "entanglement generation, and tailored heralding to reduce remote entanglement infidelities.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Entanglement is established between distant nodes in a\n",
      "quantum network through a process called entanglement swapping, mediated by a third node. This involves first generating entanglement between\n",
      "neighboring nodes using a single-photon protocol, then performing an entanglement swapping protocol on the middle node to establish entanglement\n",
      "between the distant nodes.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Bell-state measurement (BSM) plays a crucial role\n",
      "in quantum teleportation as it allows the receiver to determine the state of the qubit being teleported based on their measurement results and the\n",
      "shared entangled state between the sender and receiver. By performing a joint BSM on the sender's part of the entangled state and the qubit state to\n",
      "be teleported, the state is recovered on the receiving node through a gate operation conditioned on the BSM outcome. This process enables the quantum\n",
      "information to be transferred without being affected by losses in the connecting channels or intermediate nodes.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum\n",
      "teleportation is considered advantageous over traditional communication methods in quantum networks because it allows for the reliable transfer of\n",
      "quantum information between distant nodes without the need for physical carriers, making it insensitive to loss in connecting photonic channels and on\n",
      "intermediate nodes. It also enables unconditional teleportation through the use of a deterministic Bell-state measurement and real-time feed-forward.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The innovations that have improved the fidelity\n",
      "and reliability of quantum teleportation include improvements in memory qubit readout and protection during entanglement generation, real-time\n",
      "rejection of false heralding signals, and the development of an improved optical interface for the communication qubit. Additionally, further\n",
      "improvements such as integrating multi-pulse memory decoupling sequences and extending current schemes for use in deployed fiber networks are planned\n",
      "for future work.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main challenges associated\n",
      "with extending quantum teleportation beyond directly connected nodes include the demanding requirements on the pre-shared remote entanglement, joint\n",
      "qubit readout, and coherence times.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Memory qubits play a crucial role in the\n",
      "process of quantum teleportation in a network by serving as a storage medium for the entangled state between neighboring nodes. During the\n",
      "teleportation protocol, the input qubit state is prepared on one of the memory qubits, which is then teleported to another node through the exchange\n",
      "of classical information and local operations. Additionally, memory qubits must be reliably preserved during the teleportation process to maintain the\n",
      "entangled state between nodes. This is achieved through techniques such as error correction and active coherence protection from the spin bath.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Potential future applications of quantum\n",
      "teleportation in quantum networks include executing and testing quantum algorithms and protocols through platform-independent control software,\n",
      "improving phase stabilization, and extending current schemes for use in deployed fiber networks. Additionally, quantum teleportation can enable more\n",
      "complex protocols and applications.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Real-time feed-forward enables unconditional quantum\n",
      "teleportation by allowing state transfer each time a qubit state is inserted into the teleporter. However, it requires perfect entanglement, Bell\n",
      "state measurements, and coherence times for enabling real-time feed-forward, which has prevented its realization beyond directly connected stationary\n",
      "network nodes so far. The presence of various error sources such as imperfect Bell states, dephasing, depolarizing noise, and readout errors also\n",
      "affect the teleportation process.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      " The technical\n",
      "advancements required to realize efficient quantum teleportation between non-neighbouring nodes include improvements in the generation and maintenance\n",
      "of remote entanglement, joint qubit readout, and coherence times. Additionally, real-time rejection of false heralding signals and tailored heralding\n",
      "are necessary to reduce remote entanglement infidelities. Innovations in qubit readout procedures, active memory qubit protection during entanglement\n",
      "generation, and the use of a deterministic Bell state measurement (BSM) combined with real-time feed-forward enable unconditional teleportation.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n",
      "/content/papers/Qubit teleportation.pdf\n"
     ]
    }
   ],
   "source": [
    "# Qubit Teleportation Questions\n",
    "\n",
    "questions_file_path = '/content/questions/QubitTeleportationQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KK06Y1woEm62",
    "outputId": "f4a0407b-a057-40ef-9053-9d9c7de618bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What are the main components used in quantum networks for teleportation?\n",
      " response:  The main components used in quantum networks for\n",
      "teleportation include optically connected nodes, a teleporter in the form of a pre-shared entangled state, and a memory qubit for storing the\n",
      "teleported qubit state. Additionally, there are key innovations such as improved qubit readout procedures, active memory qubit protection during\n",
      "entanglement generation, and tailored heralding to reduce remote entanglement infidelities.\n",
      "\n",
      "\n",
      "Q1: How is entanglement established between distant nodes in a quantum network?\n",
      " response:  Entanglement is established between distant nodes in a\n",
      "quantum network through a process called entanglement swapping, mediated by a third node. This involves first generating entanglement between\n",
      "neighboring nodes using a single-photon protocol, then performing an entanglement swapping protocol on the middle node to establish entanglement\n",
      "between the distant nodes.\n",
      "\n",
      "\n",
      "Q2: What role does the Bell-state measurement (BSM) play in quantum teleportation?\n",
      " response:  The Bell-state measurement (BSM) plays a crucial role\n",
      "in quantum teleportation as it allows the receiver to determine the state of the qubit being teleported based on their measurement results and the\n",
      "shared entangled state between the sender and receiver. By performing a joint BSM on the sender's part of the entangled state and the qubit state to\n",
      "be teleported, the state is recovered on the receiving node through a gate operation conditioned on the BSM outcome. This process enables the quantum\n",
      "information to be transferred without being affected by losses in the connecting channels or intermediate nodes.\n",
      "\n",
      "\n",
      "Q3: Why is quantum teleportation considered advantageous over traditional communication methods in quantum networks?\n",
      " response:  Quantum\n",
      "teleportation is considered advantageous over traditional communication methods in quantum networks because it allows for the reliable transfer of\n",
      "quantum information between distant nodes without the need for physical carriers, making it insensitive to loss in connecting photonic channels and on\n",
      "intermediate nodes. It also enables unconditional teleportation through the use of a deterministic Bell-state measurement and real-time feed-forward.\n",
      "\n",
      "\n",
      "Q4: What innovations have improved the fidelity and reliability of quantum teleportation?\n",
      " response:  The innovations that have improved the fidelity\n",
      "and reliability of quantum teleportation include improvements in memory qubit readout and protection during entanglement generation, real-time\n",
      "rejection of false heralding signals, and the development of an improved optical interface for the communication qubit. Additionally, further\n",
      "improvements such as integrating multi-pulse memory decoupling sequences and extending current schemes for use in deployed fiber networks are planned\n",
      "for future work.\n",
      "\n",
      "\n",
      "Q5: What challenges are associated with extending quantum teleportation beyond directly connected nodes?\n",
      " response:  The main challenges associated\n",
      "with extending quantum teleportation beyond directly connected nodes include the demanding requirements on the pre-shared remote entanglement, joint\n",
      "qubit readout, and coherence times.\n",
      "\n",
      "\n",
      "Q6: How do memory qubits contribute to the process of quantum teleportation in a network?\n",
      " response:  Memory qubits play a crucial role in the\n",
      "process of quantum teleportation in a network by serving as a storage medium for the entangled state between neighboring nodes. During the\n",
      "teleportation protocol, the input qubit state is prepared on one of the memory qubits, which is then teleported to another node through the exchange\n",
      "of classical information and local operations. Additionally, memory qubits must be reliably preserved during the teleportation process to maintain the\n",
      "entangled state between nodes. This is achieved through techniques such as error correction and active coherence protection from the spin bath.\n",
      "\n",
      "\n",
      "Q7: What are potential future applications of quantum teleportation in quantum networks?\n",
      " response:  Potential future applications of quantum\n",
      "teleportation in quantum networks include executing and testing quantum algorithms and protocols through platform-independent control software,\n",
      "improving phase stabilization, and extending current schemes for use in deployed fiber networks. Additionally, quantum teleportation can enable more\n",
      "complex protocols and applications.\n",
      "\n",
      "\n",
      "Q8: How does real-time feed-forward impact the process of quantum teleportation?\n",
      " response:  Real-time feed-forward enables unconditional quantum\n",
      "teleportation by allowing state transfer each time a qubit state is inserted into the teleporter. However, it requires perfect entanglement, Bell\n",
      "state measurements, and coherence times for enabling real-time feed-forward, which has prevented its realization beyond directly connected stationary\n",
      "network nodes so far. The presence of various error sources such as imperfect Bell states, dephasing, depolarizing noise, and readout errors also\n",
      "affect the teleportation process.\n",
      "\n",
      "\n",
      "Q9: What technical advancements are needed to realize efficient quantum teleportation between non-neighbouring nodes?\n",
      " response:  The technical\n",
      "advancements required to realize efficient quantum teleportation between non-neighbouring nodes include improvements in the generation and maintenance\n",
      "of remote entanglement, joint qubit readout, and coherence times. Additionally, real-time rejection of false heralding signals and tailored heralding\n",
      "are necessary to reduce remote entanglement infidelities. Innovations in qubit readout procedures, active memory qubit protection during entanglement\n",
      "generation, and the use of a deterministic Bell state measurement (BSM) combined with real-time feed-forward enable unconditional teleportation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKJiJ4xMEnKG"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'QubitTeleportationQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljd8D_p-EnYa"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'QubitTeleportationQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gGG_6CtE3H9"
   },
   "source": [
    "###Variance Based Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiO0mcg-E3ly",
    "outputId": "2b4a3830-8b2e-4871-a58f-689fb7bb045a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The focus of variance-based sensitivity analysis in\n",
      "quantum memory is to shed light on the sensitivity of an individual quantum memory implementation to device-specific fluctuations and drift, as well\n",
      "as on the intrinsic sensitivity of different physical quantum memory protocols. It provides a complete picture of the system performance landscape\n",
      "around a central point of input parameters and allows for identification of which input parameters are most sensitive globally. Additionally, it\n",
      "probes whether correlations exist between parameters, which can be leveraged to allow for acceptable system performance at nonoptimal parameter\n",
      "values.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Λ-type quantum memory refers to a specific type of quantum memory used in optics,\n",
      "particularly in the context of atomic ensembles. It gets its name from the shape of the energy level scheme involved in the memory process, which\n",
      "forms a lambda (\"Λ\") configuration.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Control field parameters are crucial in Λ-type quantum\n",
      "memory systems because they determine the interaction between the memory qubits and the control fields, which are used to manipulate and store quantum\n",
      "information. Proper setting and stability of control field parameters are essential for achieving efficient and reliable operation of the quantum\n",
      "memory.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shot-to-shot fluctuations reduce the efficiency of quantum\n",
      "memory protocols, but the extent of reduction depends on the specific protocol being used. The study finds that the absorb-then-transfer, ATS, and EIT\n",
      "protocols exhibit different levels of stability against shot-to-shot fluctuations, with the EIT and ATS protocols being significantly more stable than\n",
      "the absorb-then-transfer protocol.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The optical control field plays a crucial role in the memory\n",
      "interaction by defining the shape and parameters of the control field used in the memory process. These parameters include the Gaussian control field\n",
      "pulse area, delay relative to the signal field, and duration, among others. The optimization of these parameters is assumed to have been achieved to\n",
      "maximize memory efficiency. However, even with optimized control fields, there can still be sensitivity to drift or improper settings of the control\n",
      "field parameters, which can affect memory performance. This sensitivity is investigated through one-at-a-time analyses such as Optimal Adjustment\n",
      "Theory (OAT) and Sobol' analysis to understand how different control field parameters impact memory efficiency and guide the development of physical\n",
      "intuition for controlling and optimizing the memory interaction.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Longer timescale drifts of control field parameters in quantum memory can\n",
      "impact its performance by affecting the memory efficiency. The paper discusses the findings that the sensitivity to drift depends on the specific\n",
      "quantum memory protocol being used.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Memory sensitivity analysis is important because it helps determine how a\n",
      "system's performance, such as memory efficiency, is affected by fluctuations in its input parameters while keeping internal system parameters fixed.\n",
      "Low sensitivity indicates a larger acceptable region of control field phase space where acceptable memory performance can be achieved, making the\n",
      "memory robust to experimental drift. The tools developed in this work are also applicable to other level systems and related techniques beyond\n",
      "resonant Lambda1-type memory protocols.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text discusses the use of variance-based sensitivity\n",
      "analysis for studying memory sensitivity in quantum systems. Specifically, it calculates the sensitivity index for different control field parameters\n",
      "such as delay and pulse duration using the Sobol' method. It also mentions that other types of sensitivity, like fidelity sensitivity, can be analyzed\n",
      "using the same framework but may have experimental considerations beyond those discussed in the text.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:485: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:490: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Gaussian control field relates to memory performance through\n",
      "its optimization for maximizing memory efficiency. Its parameters, such as pulse area, delay, and duration, define a temporal envelope that shapes the\n",
      "interaction between the memory and the control field. Memory efficiency fluctuations can arise due to variations in these control field parameters, as\n",
      "well as correlations between them. The sensitivity of memory performance to these control field parameters can be quantified using methods like one-\n",
      "at-a-time analysis or Sobol' sensitivity indices. Additionally, the overlap felidity between optimal control fields defines how similar they are, and\n",
      "regions with less overlap felidity, such as the absorb-then-transfer protocol, are more sensitive to fluctuations in memory parameters.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      " The findings on quantum memory sensitivity have\n",
      "practical ramifications for quantum memory experiments. A more robust quantum memory, which is less sensitive to experimental fluctuations and drift,\n",
      "is more useful for real-world quantum applications. The study provides insights into the sensitivity of different physical quantum memory protocols\n",
      "and helps identify ways to improve their robustness against experimental noise.\n",
      "\n",
      "\n",
      "Sources:\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n",
      "/content/papers/Variance-based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Variance Based Questions\n",
    "\n",
    "questions_file_path = '/content/questions/VarianceQuestions.csv'\n",
    "Q_dictionary = create_question_dict(questions_file_path)\n",
    "# for q in Q_dictionary:\n",
    "#   print(q)\n",
    "# QA_dictionary = generate_qa_dict(Q_dictionary)\n",
    "\n",
    "for k in Q_dictionary.keys():\n",
    "  # print(str(k))\n",
    "  query = str(k)\n",
    "  response = qa_chain(query)\n",
    "  final_res = process_generated_response(response)\n",
    "  Q_dictionary.update({k : final_res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czXJQDQGE313",
    "outputId": "51874d95-47d0-410f-c181-ea9571a19015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: What is the focus of variance-based sensitivity analysis in quantum memory?\n",
      " response:  The focus of variance-based sensitivity analysis in\n",
      "quantum memory is to shed light on the sensitivity of an individual quantum memory implementation to device-specific fluctuations and drift, as well\n",
      "as on the intrinsic sensitivity of different physical quantum memory protocols. It provides a complete picture of the system performance landscape\n",
      "around a central point of input parameters and allows for identification of which input parameters are most sensitive globally. Additionally, it\n",
      "probes whether correlations exist between parameters, which can be leveraged to allow for acceptable system performance at nonoptimal parameter\n",
      "values.\n",
      "\n",
      "\n",
      "Q1: What does Λ-type quantum memory refer to?\n",
      " response:  Λ-type quantum memory refers to a specific type of quantum memory used in optics,\n",
      "particularly in the context of atomic ensembles. It gets its name from the shape of the energy level scheme involved in the memory process, which\n",
      "forms a lambda (\"Λ\") configuration.\n",
      "\n",
      "\n",
      "Q2: Why are control field parameters crucial in Λ-type quantum memory systems?\n",
      " response:  Control field parameters are crucial in Λ-type quantum\n",
      "memory systems because they determine the interaction between the memory qubits and the control fields, which are used to manipulate and store quantum\n",
      "information. Proper setting and stability of control field parameters are essential for achieving efficient and reliable operation of the quantum\n",
      "memory.\n",
      "\n",
      "\n",
      "Q3: How does shot-to-shot fluctuation impact quantum memory performance?\n",
      " response:  Shot-to-shot fluctuations reduce the efficiency of quantum\n",
      "memory protocols, but the extent of reduction depends on the specific protocol being used. The study finds that the absorb-then-transfer, ATS, and EIT\n",
      "protocols exhibit different levels of stability against shot-to-shot fluctuations, with the EIT and ATS protocols being significantly more stable than\n",
      "the absorb-then-transfer protocol.\n",
      "\n",
      "\n",
      "Q4: What role does the optical control field play in the memory interaction?\n",
      " response:  The optical control field plays a crucial role in the memory\n",
      "interaction by defining the shape and parameters of the control field used in the memory process. These parameters include the Gaussian control field\n",
      "pulse area, delay relative to the signal field, and duration, among others. The optimization of these parameters is assumed to have been achieved to\n",
      "maximize memory efficiency. However, even with optimized control fields, there can still be sensitivity to drift or improper settings of the control\n",
      "field parameters, which can affect memory performance. This sensitivity is investigated through one-at-a-time analyses such as Optimal Adjustment\n",
      "Theory (OAT) and Sobol' analysis to understand how different control field parameters impact memory efficiency and guide the development of physical\n",
      "intuition for controlling and optimizing the memory interaction.\n",
      "\n",
      "\n",
      "Q5: How do longer timescale drifts affect quantum memory?\n",
      " response:  Longer timescale drifts of control field parameters in quantum memory can\n",
      "impact its performance by affecting the memory efficiency. The paper discusses the findings that the sensitivity to drift depends on the specific\n",
      "quantum memory protocol being used.\n",
      "\n",
      "\n",
      "Q6: What is the significance of memory sensitivity analysis?\n",
      " response:  Memory sensitivity analysis is important because it helps determine how a\n",
      "system's performance, such as memory efficiency, is affected by fluctuations in its input parameters while keeping internal system parameters fixed.\n",
      "Low sensitivity indicates a larger acceptable region of control field phase space where acceptable memory performance can be achieved, making the\n",
      "memory robust to experimental drift. The tools developed in this work are also applicable to other level systems and related techniques beyond\n",
      "resonant Lambda1-type memory protocols.\n",
      "\n",
      "\n",
      "Q7: What experimental techniques are used to analyze memory sensitivity?\n",
      " response:  The text discusses the use of variance-based sensitivity\n",
      "analysis for studying memory sensitivity in quantum systems. Specifically, it calculates the sensitivity index for different control field parameters\n",
      "such as delay and pulse duration using the Sobol' method. It also mentions that other types of sensitivity, like fidelity sensitivity, can be analyzed\n",
      "using the same framework but may have experimental considerations beyond those discussed in the text.\n",
      "\n",
      "\n",
      "Q8: How does the Gaussian control field relate to memory performance?\n",
      " response:  The Gaussian control field relates to memory performance through\n",
      "its optimization for maximizing memory efficiency. Its parameters, such as pulse area, delay, and duration, define a temporal envelope that shapes the\n",
      "interaction between the memory and the control field. Memory efficiency fluctuations can arise due to variations in these control field parameters, as\n",
      "well as correlations between them. The sensitivity of memory performance to these control field parameters can be quantified using methods like one-\n",
      "at-a-time analysis or Sobol' sensitivity indices. Additionally, the overlap felidity between optimal control fields defines how similar they are, and\n",
      "regions with less overlap felidity, such as the absorb-then-transfer protocol, are more sensitive to fluctuations in memory parameters.\n",
      "\n",
      "\n",
      "Q9: What practical ramifications do the findings on quantum memory sensitivity have?\n",
      " response:  The findings on quantum memory sensitivity have\n",
      "practical ramifications for quantum memory experiments. A more robust quantum memory, which is less sensitive to experimental fluctuations and drift,\n",
      "is more useful for real-world quantum applications. The study provides insights into the sensitivity of different physical quantum memory protocols\n",
      "and helps identify ways to improve their robustness against experimental noise.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (qstn, resp) in enumerate(Q_dictionary.items()):\n",
    "  print(f\"Q{index}: {qstn}\\n response: {resp}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWVjvnyoE4Np"
   },
   "outputs": [],
   "source": [
    "json_file_name = 'VarianceQA.json'\n",
    "df_json = pd.DataFrame([Q_dictionary])\n",
    "df_json.to_json(json_file_name, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGuOvCPHE4hy"
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'VarianceQA.csv'\n",
    "df_csv = pd.DataFrame([Q_dictionary])\n",
    "df_csv.to_csv(csv_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002041900b604adab98d696c7198ed0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00e1754973524518bb379925783225df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_280bcb15edaf43b09d8254700b4d0754",
       "IPY_MODEL_7f5bc57569ca43a68b66f54cc4a201b4",
       "IPY_MODEL_6e3dfa3dd4434e0c8d9926c0a06820b8"
      ],
      "layout": "IPY_MODEL_b3b03b70cb7347a4863da86daf5c69f1"
     }
    },
    "01dd1005d2354f1ebd4c61003aee2d32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0366b3c26cdb4b7d8fab9f3c1de29562": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "037a554ccbbc41ea9e970157bc7eb0d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_002041900b604adab98d696c7198ed0e",
      "placeholder": "​",
      "style": "IPY_MODEL_64000322383c4f57acff83622c7b358d",
      "value": "tokenizer.json: 100%"
     }
    },
    "046654fefe8344f9a2b3efd928c16af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0498c67074714e4cbe3f0e1b07295907": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05cd8e54f2e94faebf4c2e227fae1e5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06f45986391047708d004c3d6056aa32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e37aef2e954c07ab54fbf8bacc7748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a7027ae7f7149bc9067e986c00de3fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ade2d279bcf41839323bb1273f29c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_426440bd10d0467cb8eea069b1b8d905",
      "max": 4999819336,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_927a9a4061bf4b6fa5f0d252bd131c14",
      "value": 4999819336
     }
    },
    "0b22492ea20e4ff6ac13c70222956830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba424ec7d1ef45cc900fea9c24a74898",
       "IPY_MODEL_f679180fd4b7473d94aeacab41bbf015",
       "IPY_MODEL_af940397c311455ea2ffa7b343880c1a"
      ],
      "layout": "IPY_MODEL_d79442f915ea4c9ca7cb22b96bac263d"
     }
    },
    "0b5f660ff9274e449ce9fb87249aaa29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cde2a34204444fa948f4e2d4b9ba947": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_199a289278eb40268a4377e4add2a611",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fec043c725324aa3abb94bdab744b39e",
      "value": 366
     }
    },
    "0cf331d17cd4407bb73a6fb012b4ba92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0da1553d8040458ba762fef23a187d7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddbe6fe9fda54943b9621d2ebc023efc",
      "placeholder": "​",
      "style": "IPY_MODEL_a53038acaf564a08881533914b9df9d7",
      "value": "README.md: 100%"
     }
    },
    "0e7246f9a63e43f6b7890afd2764f384": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e759d7fdb1f4700b28e086f1e34e549": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ef16a9668a94e748d2d90eae4d017b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1100bd1a3d6a470cbb0040cdf871e130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1261f37570bf4257ad55024081999409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_e08f4ad0e0ef4ab3976d987aa385d61c",
      "style": "IPY_MODEL_3f9a074ac29f4ba5842218710d4103ec",
      "tooltip": ""
     }
    },
    "1293beb5c95e4a1c9633630f5bb9bea1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "13758587ac7748cda28690c97cdf77f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e759d7fdb1f4700b28e086f1e34e549",
      "placeholder": "​",
      "style": "IPY_MODEL_08e37aef2e954c07ab54fbf8bacc7748",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "137b419e6bb44db0bd8fc9b28901a2b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14af104bf8a8453d8108ac7721c83315": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14bdf801594d495eb0cb120ef5bddb25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1806ecbe10c46d8b0c94f496d09795a",
      "placeholder": "​",
      "style": "IPY_MODEL_cbecbef698924d93bd037127e1b159df",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "15513d3703a243f2a04f33aa1402d476": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15ea0120d6744e6dacea1cbcd1a60287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1731fec268d94ca7862f04b14cbf5a39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bb2d1e64cde4aa3bbe768b32e66c80a",
      "placeholder": "​",
      "style": "IPY_MODEL_36a9b37ffd05474c970f60142a22f4e5",
      "value": " 232k/232k [00:00&lt;00:00, 3.80MB/s]"
     }
    },
    "1781208ee1244bfaa2aecb9dadd9f98e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1887216f85004954b9634c9124b1bac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d696d410a7df45b1b242c8714dccb940",
      "placeholder": "​",
      "style": "IPY_MODEL_b88ffe2f67584f05bcefa2c04084b274",
      "value": ""
     }
    },
    "188bf46324ef4ac487aa6076a7b73c8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1967b3e16a994318882f09a68b5cfd9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "196a3a27115647cf8f1734d17116b756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8df34d2eea884ca18bab1c40ae703caa",
      "placeholder": "​",
      "style": "IPY_MODEL_620d5322a17e403eb005a1782886b60f",
      "value": " 4.94G/4.94G [00:32&lt;00:00, 235MB/s]"
     }
    },
    "199a289278eb40268a4377e4add2a611": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19c35688e3194214926658520a45f169": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a1495f8a19d4c81b737dd5c2e47b857": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b30775c9c204ca082fdd88740e648ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b3e322741324e6994ba7306b394f815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1baed2ed7102478e804bb2ffb29c1985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9fe7d2e01fa4b419b09e21738a6918f",
       "IPY_MODEL_7d151b58322b47d0ac1b586fc08a24cb",
       "IPY_MODEL_1731fec268d94ca7862f04b14cbf5a39"
      ],
      "layout": "IPY_MODEL_a52ad630d7fc481b9402440158900289"
     }
    },
    "1c60b47bff9d44eeb67d8e0b5c2f7d15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_037a554ccbbc41ea9e970157bc7eb0d7",
       "IPY_MODEL_2917d9c755be4397b03ad2f78f1abf79",
       "IPY_MODEL_f1bead4e812643db9e3c0af8e876f386"
      ],
      "layout": "IPY_MODEL_3c5cb0bff86d474bab14d1e8452c03fe"
     }
    },
    "1e886f29300f441d99273ce878d3289a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f01fbca169c49b68b10299e26d81523": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_137b419e6bb44db0bd8fc9b28901a2b4",
      "placeholder": "​",
      "style": "IPY_MODEL_15ea0120d6744e6dacea1cbcd1a60287",
      "value": " 366/366 [00:00&lt;00:00, 27.9kB/s]"
     }
    },
    "1f281e3364fd4573b249b9df410c151f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52698b73fe2c4c99bdb1bf05d6568766",
      "placeholder": "​",
      "style": "IPY_MODEL_d9aa6637755240d2aa28736296f0acf2",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "1f3ac3b1fe9d41fb93b836f735072703": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fc9dec7f7494d1eabd03c016e09da60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b8b2cb8804407cbce682dd97cce234": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "222f8492a9cd42a1bebd500c51fa0bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8cea687662b427ba5fd5d998d8cb5b3",
      "max": 596,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdccd1cdcf354119a16cb430cea5643a",
      "value": 596
     }
    },
    "23bf45437ce6409184b1c0e466a7101e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5fbbb906848b4c4e84995f5131bb09da",
       "IPY_MODEL_1f281e3364fd4573b249b9df410c151f",
       "IPY_MODEL_521bc82b39b34f4a8bea33cc0d27a8af",
       "IPY_MODEL_edd49ea938504249bc7548060f782edf"
      ],
      "layout": "IPY_MODEL_1293beb5c95e4a1c9633630f5bb9bea1"
     }
    },
    "2463bd8fae3145c18ea6b31689881628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24f0aa7cae3740c4b11aede5b79dfce8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "275c5a6424b34d21990382674a3631da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27c4963d070b4c38a5875e71406fd96e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280bcb15edaf43b09d8254700b4d0754": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f83ed961262c46189f7b5f3a38ecb12f",
      "placeholder": "​",
      "style": "IPY_MODEL_aa4d595f29b649699b206528f88fce04",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "28751b14571e4946abe862996342f7b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_964d30c202fb492c9fe60e3dd1af844e",
      "placeholder": "​",
      "style": "IPY_MODEL_a688691934e9497ca378c472a8460d20",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "28f930cc65ea4e9cbaeba3d4160b3f67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2917d9c755be4397b03ad2f78f1abf79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f72a4f970502478c83fa1d0174eb263c",
      "max": 1795303,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_60d00a2405cf4350ac25aa8907d4a0c4",
      "value": 1795303
     }
    },
    "2a74a36216654bb88b7c0f43289db975": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2aa5bee586eb45879006f2420d2a3628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2abd5b29aff74bc18180527e8c4a0a93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24f0aa7cae3740c4b11aede5b79dfce8",
      "placeholder": "​",
      "style": "IPY_MODEL_8ee5331d9fea4df5a5a4b43f66f3af72",
      "value": " 777/777 [00:00&lt;00:00, 55.0kB/s]"
     }
    },
    "2adf960b5bc04fa68fb0986d8335363f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c6033d8cbf5411ca3bfa297c5f44ef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5b68083538f43418f2cd1c03224d086",
       "IPY_MODEL_6cdc3b2fc088489487261958c002b01f",
       "IPY_MODEL_f543043a87fc4c64a8c882d18e0eb2dc"
      ],
      "layout": "IPY_MODEL_20b8b2cb8804407cbce682dd97cce234"
     }
    },
    "2c902a30db4a4f66b98d6eb66e6327f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd8a972f36474e559a2d4fbb48c5ba30",
      "placeholder": "​",
      "style": "IPY_MODEL_01dd1005d2354f1ebd4c61003aee2d32",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "2c9237761bf84fee8650be3da0c08494": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2eccbfca06b848f1945a27c1dbe36249": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "307eb842339c4fdc94de74509906b632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_961e0fc3f38444be85000d38e6a6d1fc",
       "IPY_MODEL_7cce2628fa72424782939699cc18edc9",
       "IPY_MODEL_97c0636c927c4186b8679d25b7176bb4"
      ],
      "layout": "IPY_MODEL_4986a6a89f8a41888c67757b5bc79862"
     }
    },
    "33524de32bda4d65b24cf8b26430f1cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3400ff0f60634591a5867a1b762d78d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34797c85fea64a51ab8fb3bb045688c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35a51cc1e68c46938e687c39a478b5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35fda595e1f548b49774690bf3f6b912": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36a9b37ffd05474c970f60142a22f4e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ade9c7856d744c3af4ed7b335b7189e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bbacd04642045129f5a5fcdc30630a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e886f29300f441d99273ce878d3289a",
      "placeholder": "​",
      "style": "IPY_MODEL_2c9237761bf84fee8650be3da0c08494",
      "value": "Downloading shards: 100%"
     }
    },
    "3bf27823ccdb4a938bbfb7c2c1dc618a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c5cb0bff86d474bab14d1e8452c03fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cf6b832476d48f49f7e8eb914a48337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9a074ac29f4ba5842218710d4103ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "41f91e0c26d247d294ce7ca9ddf330d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "426440bd10d0467cb8eea069b1b8d905": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43fce2542dd8448cb2c48e99f0a80d6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "448e57cc2ba84a77a605397c5fd2b51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0da1553d8040458ba762fef23a187d7e",
       "IPY_MODEL_92668dd987054aa599d07f40097d4dbe",
       "IPY_MODEL_c1b5cdd231d04962ac6dad0e1d60e29c"
      ],
      "layout": "IPY_MODEL_dcd7aed591664d1d8873dbff4304621f"
     }
    },
    "46716d7d7eb74d0ca5e857c425239436": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4676f34c60d0420996c4a3d1ff571e29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47357209fd534059814ed5fc2eeed1fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4986a6a89f8a41888c67757b5bc79862": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4addf254069742238761baa022adc5cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bb2d1e64cde4aa3bbe768b32e66c80a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4de27b19b17840458dcce998bc235f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9846333f66f4836a7c5c81ed2e69d45",
      "max": 711396,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_666682c677e4471f83d187ca70dc4ef8",
      "value": 711396
     }
    },
    "51b7c70dc4b9450487f8d475980b6e42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51e3d40641af49dd9ce01f5c588b930a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a84fd92fcbe645b789c251664db8f0c5",
      "placeholder": "​",
      "style": "IPY_MODEL_d15dd441080f4b06a8246508ab0a4fa4",
      "value": "generation_config.json: 100%"
     }
    },
    "521bc82b39b34f4a8bea33cc0d27a8af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccdbca3b36854de4aabea2706d26ae7f",
      "placeholder": "​",
      "style": "IPY_MODEL_b76acac829c541ad866ea86517c6eebb",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "5260c96059654a80abaa37d36707b7ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b0ae08a278642239b84ee6ae17229a3",
      "placeholder": "​",
      "style": "IPY_MODEL_f06b79cadbe148f0a6d9326dc0c124ca",
      "value": "model-00003-of-00003.safetensors: 100%"
     }
    },
    "52698b73fe2c4c99bdb1bf05d6568766": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52a444ea63e0441a8ea79c936d52eb0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55a39190b648444d99bc71fb0d51cbc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55b666492ddd41b3b95043b62084a379": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_188bf46324ef4ac487aa6076a7b73c8c",
      "placeholder": "​",
      "style": "IPY_MODEL_a6267fbeddf944a7be6f1ec9abb0cea2",
      "value": " 190/190 [00:00&lt;00:00, 11.6kB/s]"
     }
    },
    "5673b053c6e940c78e849fc9106b8edd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "569d14f9d172430eba099f90a3b1a3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fc0c999621242fc9db31cdd3765ac5f",
      "placeholder": "​",
      "style": "IPY_MODEL_1b3e322741324e6994ba7306b394f815",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "573236e4640f4304b1727190f2777732": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "574f4c8c94024bc5ba1946a4205b38ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ad95190c1e4ded9481a417e1fa2432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffe40902c949497b88647e826c239475",
      "placeholder": "​",
      "style": "IPY_MODEL_a49c7f526b3347f4af52519dc9d2074d",
      "value": " 125/125 [00:00&lt;00:00, 9.93kB/s]"
     }
    },
    "589fa8ee44a6409a838477e50e55e57d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59fd6bdb84b3422085a4e701f2f930a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a98451db7b14bdbb54f9eeab5f8da19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ade9c7856d744c3af4ed7b335b7189e",
      "placeholder": "​",
      "style": "IPY_MODEL_52a444ea63e0441a8ea79c936d52eb0a",
      "value": "model-00001-of-00003.safetensors: 100%"
     }
    },
    "5cacc1e1f6da4bacbac7a7bcd084e2d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d449ee019d747baaabf5326c3a4259c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4addf254069742238761baa022adc5cd",
      "placeholder": "​",
      "style": "IPY_MODEL_275c5a6424b34d21990382674a3631da",
      "value": "config.json: 100%"
     }
    },
    "5d9984af84744606988707c50d4506c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e6486e79e7d4a2c8a42eb6c0786671f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e7580e0d388450e9696c6664f01d87c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_724d70a5ea56415485c194173296b3e0",
      "placeholder": "​",
      "style": "IPY_MODEL_e177e8e2f26a4e94a6c917459b26621f",
      "value": "tokenizer.json: 100%"
     }
    },
    "5fbbb906848b4c4e84995f5131bb09da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f568b16fcaaf482b934cd0d13917d1c1",
      "placeholder": "​",
      "style": "IPY_MODEL_bcbd136c794a423798e15d3c42bcb57a",
      "value": "Token is valid (permission: write)."
     }
    },
    "60d00a2405cf4350ac25aa8907d4a0c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6186be01eb8f47df9762083f6ff80749": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61e4132b12f2493489df1e2535ddd445": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "620d5322a17e403eb005a1782886b60f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62a498a5f2e940b49b92589ffebaee0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e7580e0d388450e9696c6664f01d87c",
       "IPY_MODEL_4de27b19b17840458dcce998bc235f6f",
       "IPY_MODEL_aec5c9b583364155b4e8e4a75c233b86"
      ],
      "layout": "IPY_MODEL_1967b3e16a994318882f09a68b5cfd9a"
     }
    },
    "64000322383c4f57acff83622c7b358d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "666682c677e4471f83d187ca70dc4ef8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6bf16cc4619f417e89469699865ea4e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ca97e66d96e4d0ea5a97d36746ec95e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b296da6b5deb4eb0921abf2d24fa37c4",
       "IPY_MODEL_222f8492a9cd42a1bebd500c51fa0bd5",
       "IPY_MODEL_cf174d180b554d1aa4dec34999bb1c88"
      ],
      "layout": "IPY_MODEL_d6d709829cd74ad3a9c8c6829a81f9b2"
     }
    },
    "6cdc3b2fc088489487261958c002b01f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba491ed1463c4b398cca28c11a140c4f",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5673b053c6e940c78e849fc9106b8edd",
      "value": 3
     }
    },
    "6d6e605d7b66433eb523cb67efb93c60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e3dfa3dd4434e0c8d9926c0a06820b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cf6b832476d48f49f7e8eb914a48337",
      "placeholder": "​",
      "style": "IPY_MODEL_984c3923b8584edaa32844052f72fb12",
      "value": " 124/124 [00:00&lt;00:00, 6.83kB/s]"
     }
    },
    "6e83e74b9fcc49c088c4f95c61cba3d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71b451fca481469ea6794666f857c99b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b0bbb0247734ca6881405c80a235215",
       "IPY_MODEL_f2221eb86b7e483fa92b822e993fd85a",
       "IPY_MODEL_ddce4c5d6ed44cda9db14a5181077474"
      ],
      "layout": "IPY_MODEL_6e83e74b9fcc49c088c4f95c61cba3d0"
     }
    },
    "724d70a5ea56415485c194173296b3e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "740bfdf0f72a45d995059196e9622ed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbd407a5d75145a2a3cbbfabc9f96c71",
      "max": 1460,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d33b2334497e48cdb5f02a8283a199f2",
      "value": 1460
     }
    },
    "7472cd2381ce45bd8998a4796bc206c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76dfd320620c4ac796d9cd493c6f83c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9557149c87bd4e45b43274a742966395",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abd90d562f724faa9456795d93fde170",
      "value": 190
     }
    },
    "778db3a016c949008d04f5c4f2a2aff9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a89a08a0c7447f8b1210dcc6bf93b6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b5f660ff9274e449ce9fb87249aaa29",
      "placeholder": "​",
      "style": "IPY_MODEL_0366b3c26cdb4b7d8fab9f3c1de29562",
      "value": "Connecting..."
     }
    },
    "7a9095f5a2064292b106e92b3be532c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_569d14f9d172430eba099f90a3b1a3bb",
       "IPY_MODEL_76dfd320620c4ac796d9cd493c6f83c3",
       "IPY_MODEL_55b666492ddd41b3b95043b62084a379"
      ],
      "layout": "IPY_MODEL_91e82d827fee40a09fd5ce99f5c183af"
     }
    },
    "7b0bbb0247734ca6881405c80a235215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80fde45cbe9e4284bd7c6be0e0f5610c",
      "placeholder": "​",
      "style": "IPY_MODEL_573236e4640f4304b1727190f2777732",
      "value": "tokenizer.model: 100%"
     }
    },
    "7b8e53eac3594dd98d36f36cc9f052d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c3e27a4c8934a0690bb64d69db07782": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c7c7e624f574219b56e6be2f8181a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ef16a9668a94e748d2d90eae4d017b0",
      "max": 777,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8500952c31b847039a71bdee2abd9199",
      "value": 777
     }
    },
    "7cce2628fa72424782939699cc18edc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a79cd4416aee42fe94ca6e5494013e95",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d83c82cdd70940ecbd96f4ca6ff9dd8b",
      "value": 52
     }
    },
    "7d00a752e6aa4cbb963ffd840af71866": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06f45986391047708d004c3d6056aa32",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ddb3a548269a48d3b4b467e780c197fb",
      "value": 111
     }
    },
    "7d151b58322b47d0ac1b586fc08a24cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa00a82a1eea4d4ab8a7518c68f0b33e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35a51cc1e68c46938e687c39a478b5a3",
      "value": 231508
     }
    },
    "7f5bc57569ca43a68b66f54cc4a201b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35fda595e1f548b49774690bf3f6b912",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9aae3babe542455281f0966564e6da77",
      "value": 124
     }
    },
    "80fde45cbe9e4284bd7c6be0e0f5610c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81e14c79bbdd40d387684e1633555dea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb63dcfb203843eeb7421ff11b5228a7",
      "max": 437955512,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_821185cfd61745ce8adc637392bf78f8",
      "value": 437955512
     }
    },
    "821185cfd61745ce8adc637392bf78f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "826e0e1ae457443899981833aa7ec5d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84a66d28b4fa4d41a14efb9ad7864310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c64786bdb6cf4254a75fbb20fb2765a2",
      "max": 25125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd80a7b9d5fe432d9809c31fe7d16259",
      "value": 25125
     }
    },
    "8500952c31b847039a71bdee2abd9199": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "86752b76123b40a7ac9850c47245d111": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "873b1c759898413fb0f13db922aa2635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e04fc4738e384cb793bd96eaa813ad03",
      "placeholder": "​",
      "style": "IPY_MODEL_d38ed742a3384fd59bcdeabe54a21a4c",
      "value": " 111/111 [00:00&lt;00:00, 8.14kB/s]"
     }
    },
    "87c75806934643a7a46518d968e83320": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8909de6fdf1543fd8c6d01f11fe375a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_778db3a016c949008d04f5c4f2a2aff9",
      "placeholder": "​",
      "style": "IPY_MODEL_59fd6bdb84b3422085a4e701f2f930a0",
      "value": " 5.00G/5.00G [00:33&lt;00:00, 194MB/s]"
     }
    },
    "89b6408181354a22a6560b509d1ec870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41f91e0c26d247d294ce7ca9ddf330d3",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0cf331d17cd4407bb73a6fb012b4ba92",
      "value": 125
     }
    },
    "8a4df7f105ae4b8d92381c308ac0f917": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8df34d2eea884ca18bab1c40ae703caa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee5331d9fea4df5a5a4b43f66f3af72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fc0c999621242fc9db31cdd3765ac5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91e82d827fee40a09fd5ce99f5c183af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92668dd987054aa599d07f40097d4dbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab22d7af38b146e8be12c67e1472d500",
      "max": 94551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98169847b24b45b3a4af94efcd6f550a",
      "value": 94551
     }
    },
    "927a9a4061bf4b6fa5f0d252bd131c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "936a018b20694922997c6bcfa37c2054": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9482d7c593474010b0a0ac44a3a81794": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94ec93a7fddb48d68ee438c65ab9357a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9557149c87bd4e45b43274a742966395": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "961e0fc3f38444be85000d38e6a6d1fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4640c0400324cce8234371c100e629c",
      "placeholder": "​",
      "style": "IPY_MODEL_a6481318fb2e48748917ee42d4b977db",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "964d30c202fb492c9fe60e3dd1af844e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9654821dc6374bcf83ecf9f116227709": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97c0636c927c4186b8679d25b7176bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba6be75453aa4dfa96f9f037b65ea445",
      "placeholder": "​",
      "style": "IPY_MODEL_d7ac435f3e1747838bd9530d9eed2ded",
      "value": " 52.0/52.0 [00:00&lt;00:00, 4.17kB/s]"
     }
    },
    "98169847b24b45b3a4af94efcd6f550a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "984c3923b8584edaa32844052f72fb12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9879926d07ee4fdc9d7b9a9c17165066": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5260c96059654a80abaa37d36707b7ab",
       "IPY_MODEL_e31dca8e9071462fbcce4065049a1fc4",
       "IPY_MODEL_ae4e065cbcd84982a2c3adac7f482663"
      ],
      "layout": "IPY_MODEL_27c4963d070b4c38a5875e71406fd96e"
     }
    },
    "9aae3babe542455281f0966564e6da77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9b0ae08a278642239b84ee6ae17229a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b7bc871a2524d39a836ab2d4504a65c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbd42fc7477449fc9fec199fc199c185",
       "IPY_MODEL_81e14c79bbdd40d387684e1633555dea",
       "IPY_MODEL_e53d692f859046cf9d6cb7a2fa709d66"
      ],
      "layout": "IPY_MODEL_51b7c70dc4b9450487f8d475980b6e42"
     }
    },
    "9bfd503d74ae4065b762864bae727130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7cfe4b7ff16427d89dec514c0aeb92d",
       "IPY_MODEL_e45070a7b6b647238075e8be25d35641",
       "IPY_MODEL_fbead7b64c7c40c190ff6cc5f0131943"
      ],
      "layout": "IPY_MODEL_1a1495f8a19d4c81b737dd5c2e47b857"
     }
    },
    "9df87b2000284e01b4db02d1666fb76f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f1a6f50d39d4df2ad173aa8d9b41e41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb84cfd19fb64242a78bb732f63316d1",
      "placeholder": "​",
      "style": "IPY_MODEL_df5120c70209444cb482bb0ef5c37318",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "a24de36882e54e1b86aee7adcd27ec1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a49c7f526b3347f4af52519dc9d2074d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a52ad630d7fc481b9402440158900289": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a53038acaf564a08881533914b9df9d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6267fbeddf944a7be6f1ec9abb0cea2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6481318fb2e48748917ee42d4b977db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a688691934e9497ca378c472a8460d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a747a39bb04546248526684c105179ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a79cd4416aee42fe94ca6e5494013e95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a84fd92fcbe645b789c251664db8f0c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8cea687662b427ba5fd5d998d8cb5b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9846333f66f4836a7c5c81ed2e69d45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa4d595f29b649699b206528f88fce04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aaa73632195c44cd9273a81b196bbb31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab22d7af38b146e8be12c67e1472d500": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab3617ffd23e40b2a9ce7936c05358ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14bdf801594d495eb0cb120ef5bddb25",
       "IPY_MODEL_89b6408181354a22a6560b509d1ec870",
       "IPY_MODEL_57ad95190c1e4ded9481a417e1fa2432"
      ],
      "layout": "IPY_MODEL_47357209fd534059814ed5fc2eeed1fd"
     }
    },
    "abd90d562f724faa9456795d93fde170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae4e065cbcd84982a2c3adac7f482663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a24de36882e54e1b86aee7adcd27ec1e",
      "placeholder": "​",
      "style": "IPY_MODEL_2adf960b5bc04fa68fb0986d8335363f",
      "value": " 4.54G/4.54G [00:23&lt;00:00, 193MB/s]"
     }
    },
    "aec5c9b583364155b4e8e4a75c233b86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e534dbbce1d944979abdcffd8e482ec8",
      "placeholder": "​",
      "style": "IPY_MODEL_5cacc1e1f6da4bacbac7a7bcd084e2d7",
      "value": " 711k/711k [00:00&lt;00:00, 2.82MB/s]"
     }
    },
    "af940397c311455ea2ffa7b343880c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1781208ee1244bfaa2aecb9dadd9f98e",
      "placeholder": "​",
      "style": "IPY_MODEL_94ec93a7fddb48d68ee438c65ab9357a",
      "value": " 349/349 [00:00&lt;00:00, 15.9kB/s]"
     }
    },
    "b162f44d2069466ea3e9103093364bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a98451db7b14bdbb54f9eeab5f8da19",
       "IPY_MODEL_e3dec87ca68845d1a41c06764cdc7d69",
       "IPY_MODEL_196a3a27115647cf8f1734d17116b756"
      ],
      "layout": "IPY_MODEL_19c35688e3194214926658520a45f169"
     }
    },
    "b296da6b5deb4eb0921abf2d24fa37c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86752b76123b40a7ac9850c47245d111",
      "placeholder": "​",
      "style": "IPY_MODEL_e5c39b3b24b04a7c967aed2338656619",
      "value": "config.json: 100%"
     }
    },
    "b3b03b70cb7347a4863da86daf5c69f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b641ab0c056b4104a40e671a0474f293": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b76acac829c541ad866ea86517c6eebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b88ffe2f67584f05bcefa2c04084b274": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba334a5029914c5c8d2ea654774632c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba424ec7d1ef45cc900fea9c24a74898": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c99a95faa5664465a83e968952c89604",
      "placeholder": "​",
      "style": "IPY_MODEL_4676f34c60d0420996c4a3d1ff571e29",
      "value": "modules.json: 100%"
     }
    },
    "ba491ed1463c4b398cca28c11a140c4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba6be75453aa4dfa96f9f037b65ea445": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb63dcfb203843eeb7421ff11b5228a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbbe9b65796d40e19a2a38f90661ea9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51e3d40641af49dd9ce01f5c588b930a",
       "IPY_MODEL_7d00a752e6aa4cbb963ffd840af71866",
       "IPY_MODEL_873b1c759898413fb0f13db922aa2635"
      ],
      "layout": "IPY_MODEL_61e4132b12f2493489df1e2535ddd445"
     }
    },
    "bbd42fc7477449fc9fec199fc199c185": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba334a5029914c5c8d2ea654774632c6",
      "placeholder": "​",
      "style": "IPY_MODEL_6bf16cc4619f417e89469699865ea4e1",
      "value": "model.safetensors: 100%"
     }
    },
    "bcbd136c794a423798e15d3c42bcb57a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd80a7b9d5fe432d9809c31fe7d16259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdccd1cdcf354119a16cb430cea5643a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be25b63783824d9cbb3b100c446476c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be98e7bc5873401e9b2d105e12d166f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc9dec7f7494d1eabd03c016e09da60",
      "placeholder": "​",
      "style": "IPY_MODEL_826e0e1ae457443899981833aa7ec5d0",
      "value": "model-00002-of-00003.safetensors: 100%"
     }
    },
    "beafbbad2e1943708c82c7fec4f048ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3bbacd04642045129f5a5fcdc30630a6",
       "IPY_MODEL_d9ff912bd9bb463f878af20f9240041a",
       "IPY_MODEL_f98abe30a22f49f5ace37e5d20ccee37"
      ],
      "layout": "IPY_MODEL_3400ff0f60634591a5867a1b762d78d2"
     }
    },
    "c1b5cdd231d04962ac6dad0e1d60e29c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28f930cc65ea4e9cbaeba3d4160b3f67",
      "placeholder": "​",
      "style": "IPY_MODEL_1100bd1a3d6a470cbb0040cdf871e130",
      "value": " 94.6k/94.6k [00:00&lt;00:00, 4.56MB/s]"
     }
    },
    "c268690ebd1743c1842173ba0e8de734": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c46c5f1c852249ef830544dc6477624d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be98e7bc5873401e9b2d105e12d166f5",
       "IPY_MODEL_0ade2d279bcf41839323bb1273f29c4c",
       "IPY_MODEL_8909de6fdf1543fd8c6d01f11fe375a7"
      ],
      "layout": "IPY_MODEL_589fa8ee44a6409a838477e50e55e57d"
     }
    },
    "c64786bdb6cf4254a75fbb20fb2765a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c64987e0fde8482c9cb51b2502218bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13758587ac7748cda28690c97cdf77f9",
       "IPY_MODEL_0cde2a34204444fa948f4e2d4b9ba947",
       "IPY_MODEL_1f01fbca169c49b68b10299e26d81523"
      ],
      "layout": "IPY_MODEL_5d9984af84744606988707c50d4506c9"
     }
    },
    "c7cfe4b7ff16427d89dec514c0aeb92d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9654821dc6374bcf83ecf9f116227709",
      "placeholder": "​",
      "style": "IPY_MODEL_f706032fe5cd4b57b6fe0895d96b2237",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "c99a95faa5664465a83e968952c89604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbecbef698924d93bd037127e1b159df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ccdbca3b36854de4aabea2706d26ae7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd1f6d87f0234baeb5e6d3143dc196aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ced3877cabf44ab5a212c9970b569a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46716d7d7eb74d0ca5e857c425239436",
      "placeholder": "​",
      "style": "IPY_MODEL_e97c7b2fdeb24d319523fc5d9314293d",
      "value": " 1.46k/1.46k [00:00&lt;00:00, 102kB/s]"
     }
    },
    "cf174d180b554d1aa4dec34999bb1c88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34797c85fea64a51ab8fb3bb045688c8",
      "placeholder": "​",
      "style": "IPY_MODEL_2aa5bee586eb45879006f2420d2a3628",
      "value": " 596/596 [00:00&lt;00:00, 30.5kB/s]"
     }
    },
    "d15dd441080f4b06a8246508ab0a4fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d240bb0ca648469583326142ceca0580": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d33b2334497e48cdb5f02a8283a199f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d38ed742a3384fd59bcdeabe54a21a4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5b68083538f43418f2cd1c03224d086": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33524de32bda4d65b24cf8b26430f1cf",
      "placeholder": "​",
      "style": "IPY_MODEL_5e6486e79e7d4a2c8a42eb6c0786671f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "d696d410a7df45b1b242c8714dccb940": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6d709829cd74ad3a9c8c6829a81f9b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d79442f915ea4c9ca7cb22b96bac263d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7ac435f3e1747838bd9530d9eed2ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d83c82cdd70940ecbd96f4ca6ff9dd8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9aa6637755240d2aa28736296f0acf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9ff912bd9bb463f878af20f9240041a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_936a018b20694922997c6bcfa37c2054",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd1f6d87f0234baeb5e6d3143dc196aa",
      "value": 3
     }
    },
    "dbd407a5d75145a2a3cbbfabc9f96c71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcd7aed591664d1d8873dbff4304621f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd8a972f36474e559a2d4fbb48c5ba30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddb3a548269a48d3b4b467e780c197fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ddbe6fe9fda54943b9621d2ebc023efc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddce4c5d6ed44cda9db14a5181077474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4420be8796a443d9eb00f2a10efe8dd",
      "placeholder": "​",
      "style": "IPY_MODEL_574f4c8c94024bc5ba1946a4205b38ce",
      "value": " 493k/493k [00:00&lt;00:00, 11.1MB/s]"
     }
    },
    "df5120c70209444cb482bb0ef5c37318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e04fc4738e384cb793bd96eaa813ad03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e08f4ad0e0ef4ab3976d987aa385d61c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e101d1fcfadc4e17a344d715922154e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e177e8e2f26a4e94a6c917459b26621f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1806ecbe10c46d8b0c94f496d09795a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1c68a8c5e0942e2a44a198f8044d76e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e31dca8e9071462fbcce4065049a1fc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be25b63783824d9cbb3b100c446476c0",
      "max": 4540516344,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_046654fefe8344f9a2b3efd928c16af9",
      "value": 4540516344
     }
    },
    "e3c87d1565fa41e4ab3efaeb4b58b58c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e3dec87ca68845d1a41c06764cdc7d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6186be01eb8f47df9762083f6ff80749",
      "max": 4943162336,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9df87b2000284e01b4db02d1666fb76f",
      "value": 4943162336
     }
    },
    "e4420be8796a443d9eb00f2a10efe8dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e45070a7b6b647238075e8be25d35641": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b8e53eac3594dd98d36f36cc9f052d2",
      "max": 72,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e3c87d1565fa41e4ab3efaeb4b58b58c",
      "value": 72
     }
    },
    "e4640c0400324cce8234371c100e629c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e534dbbce1d944979abdcffd8e482ec8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e53d692f859046cf9d6cb7a2fa709d66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f3ac3b1fe9d41fb93b836f735072703",
      "placeholder": "​",
      "style": "IPY_MODEL_0a7027ae7f7149bc9067e986c00de3fc",
      "value": " 438M/438M [00:01&lt;00:00, 241MB/s]"
     }
    },
    "e5c39b3b24b04a7c967aed2338656619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e97c7b2fdeb24d319523fc5d9314293d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea0eedfed23a4ba5876111d7068da537": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28751b14571e4946abe862996342f7b0",
       "IPY_MODEL_84a66d28b4fa4d41a14efb9ad7864310",
       "IPY_MODEL_f02af320eccf4b8bb5a8a434afdceb4e"
      ],
      "layout": "IPY_MODEL_b641ab0c056b4104a40e671a0474f293"
     }
    },
    "edd49ea938504249bc7548060f782edf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9482d7c593474010b0a0ac44a3a81794",
      "placeholder": "​",
      "style": "IPY_MODEL_aaa73632195c44cd9273a81b196bbb31",
      "value": "Login successful"
     }
    },
    "efa4e99c9283460288842dca78ad4d01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_55a39190b648444d99bc71fb0d51cbc0",
      "style": "IPY_MODEL_3bf27823ccdb4a938bbfb7c2c1dc618a",
      "value": true
     }
    },
    "f02af320eccf4b8bb5a8a434afdceb4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1c68a8c5e0942e2a44a198f8044d76e",
      "placeholder": "​",
      "style": "IPY_MODEL_87c75806934643a7a46518d968e83320",
      "value": " 25.1k/25.1k [00:00&lt;00:00, 1.53MB/s]"
     }
    },
    "f06b79cadbe148f0a6d9326dc0c124ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1bead4e812643db9e3c0af8e876f386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e101d1fcfadc4e17a344d715922154e3",
      "placeholder": "​",
      "style": "IPY_MODEL_15513d3703a243f2a04f33aa1402d476",
      "value": " 1.80M/1.80M [00:00&lt;00:00, 13.9MB/s]"
     }
    },
    "f2221eb86b7e483fa92b822e993fd85a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43fce2542dd8448cb2c48e99f0a80d6c",
      "max": 493443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b30775c9c204ca082fdd88740e648ef",
      "value": 493443
     }
    },
    "f543043a87fc4c64a8c882d18e0eb2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0498c67074714e4cbe3f0e1b07295907",
      "placeholder": "​",
      "style": "IPY_MODEL_2463bd8fae3145c18ea6b31689881628",
      "value": " 3/3 [01:09&lt;00:00, 22.70s/it]"
     }
    },
    "f568b16fcaaf482b934cd0d13917d1c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f679180fd4b7473d94aeacab41bbf015": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2eccbfca06b848f1945a27c1dbe36249",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d240bb0ca648469583326142ceca0580",
      "value": 349
     }
    },
    "f706032fe5cd4b57b6fe0895d96b2237": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f72a4f970502478c83fa1d0174eb263c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f83ed961262c46189f7b5f3a38ecb12f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8f3c77757cd4d50b3ef650330381748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d449ee019d747baaabf5326c3a4259c",
       "IPY_MODEL_7c7c7e624f574219b56e6be2f8181a12",
       "IPY_MODEL_2abd5b29aff74bc18180527e8c4a0a93"
      ],
      "layout": "IPY_MODEL_8a4df7f105ae4b8d92381c308ac0f917"
     }
    },
    "f98abe30a22f49f5ace37e5d20ccee37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05cd8e54f2e94faebf4c2e227fae1e5f",
      "placeholder": "​",
      "style": "IPY_MODEL_a747a39bb04546248526684c105179ba",
      "value": " 3/3 [01:30&lt;00:00, 28.95s/it]"
     }
    },
    "f9fe7d2e01fa4b419b09e21738a6918f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a74a36216654bb88b7c0f43289db975",
      "placeholder": "​",
      "style": "IPY_MODEL_7472cd2381ce45bd8998a4796bc206c2",
      "value": "vocab.txt: 100%"
     }
    },
    "fa00a82a1eea4d4ab8a7518c68f0b33e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb84cfd19fb64242a78bb732f63316d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbead7b64c7c40c190ff6cc5f0131943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d6e605d7b66433eb523cb67efb93c60",
      "placeholder": "​",
      "style": "IPY_MODEL_14af104bf8a8453d8108ac7721c83315",
      "value": " 72.0/72.0 [00:00&lt;00:00, 5.29kB/s]"
     }
    },
    "fec043c725324aa3abb94bdab744b39e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fec3bd85c52f46e3b8d282f48a0ff4af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c902a30db4a4f66b98d6eb66e6327f1",
       "IPY_MODEL_740bfdf0f72a45d995059196e9622ed3",
       "IPY_MODEL_ced3877cabf44ab5a212c9970b569a77"
      ],
      "layout": "IPY_MODEL_0e7246f9a63e43f6b7890afd2764f384"
     }
    },
    "ff32fb6fb1b4459caaf4a5c48b8014af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c3e27a4c8934a0690bb64d69db07782",
      "placeholder": "​",
      "style": "IPY_MODEL_c268690ebd1743c1842173ba0e8de734",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "ffe40902c949497b88647e826c239475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
