Question ID,Question,Answer
ATT1,What innovation does the Transformer model introduce?,"It relies entirely on an attention mechanism, eliminating the need for recurrence and convolutions."
ATT2,How does the Transformer model enhance training efficiency?,"It allows greater parallelization, significantly reducing training time."
ATT3,What are key components of the Transformerâ€™s encoder?,It includes layers with multi-head self-attention mechanisms and position-wise fully connected networks.
ATT4,What is the function of multi-head attention in the Transformer?,It lets the model attend to information from different subspaces at various positions.
ATT5,How does the Transformer handle sequence order?,It uses positional encodings with sine and cosine functions to maintain sequence order.
ATT6,What are the benefits of self-attention in the Transformer?,"Self-attention draws global dependencies between inputs and outputs, easing the learning of long-range dependencies."
ATT7,What is attention in Transformers?,"In Transformers, attention is a mechanism that weights the influence of different parts of the input data differently, allowing the model to focus more on relevant parts for making predictions."
ATT8,What regularization techniques are used in the Transformer?,Techniques include dropout and label smoothing to prevent overfitting.
ATT9,How did the Transformer model improve machine translation accuracy?,"The Transformer model set a new benchmark by achieving the highest BLEU scores to date on tasks, significantly surpassing the performance of prior models."
ATT10,Can the Transformer model be applied beyond machine translation?,"Yes, it was also successfully applied to English constituency parsing."
